from typing import List
from app.gpt.base import GPT
from openai import OpenAI
from app.gpt.prompt import BASE_PROMPT, AI_SUM, SCREENSHOT, LINK
from app.gpt.provider.OpenAI_compatible_provider import OpenAICompatibleProvider
from app.gpt.utils import fix_markdown, estimate_tokens, split_segments_by_tokens, merge_markdown_contents, create_chunk_summary_prompt
from app.models.gpt_model import GPTSource
from app.models.transcriber_model import TranscriptSegment
from app.utils.retry_utils import retry_on_rate_limit
from datetime import timedelta
from app.utils.logger import get_logger

logger = get_logger(__name__)


class OpenaiGPT(GPT):
    def __init__(self):
        from os import getenv
        self.api_key = getenv("OPENAI_API_KEY")
        self.base_url = getenv("OPENAI_API_BASE_URL")
        self.model=getenv('OPENAI_MODEL')
        print(self.model)
        self.client = OpenAICompatibleProvider(api_key=self.api_key, base_url=self.base_url)
        self.screenshot = False
        self.link=False

    def _format_time(self, seconds: float) -> str:
        return str(timedelta(seconds=int(seconds)))[2:]  # e.g., 03:15

    def _build_segment_text(self, segments: List[TranscriptSegment]) -> str:
        return "\n".join(
            f"{self._format_time(seg.start)} - {seg.text.strip()}"
            for seg in segments
        )

    def ensure_segments_type(self, segments) -> List[TranscriptSegment]:
        return [
            TranscriptSegment(**seg) if isinstance(seg, dict) else seg
            for seg in segments
        ]

    def create_messages(self, segments: List[TranscriptSegment], title: str, tags: str, chunk_prompt: str = ""):
        content = BASE_PROMPT.format(
            video_title=title,
            segment_text=self._build_segment_text(segments),
            tags=tags
        )
        
        # æ·»åŠ åˆ†å—å¤„ç†çš„ç‰¹æ®Špromptï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        if chunk_prompt:
            content = chunk_prompt + "\n\n" + content
            
        if self.screenshot:
            print(":éœ€è¦æˆªå›¾")
            content += SCREENSHOT
        if self.link:
            print(":éœ€è¦é“¾æ¥")
            content += LINK

        print(content)
        return [{"role": "user", "content": content + AI_SUM}]
        
    def list_models(self):
        return self.client.list_models()
        
    @retry_on_rate_limit(max_retries=3, delay=30.0, backoff_factor=1.5)
    def _summarize_chunk(self, segments: List[TranscriptSegment], title: str, tags: str, chunk_prompt: str = "") -> str:
        """å¤„ç†å•ä¸ªåˆ†å—çš„æ€»ç»“"""
        messages = self.create_messages(segments, title, tags, chunk_prompt)
        
        # ä¼°ç®—å½“å‰promptçš„tokenæ•°
        prompt_text = str(messages)
        prompt_tokens = estimate_tokens(prompt_text)
        logger.info(f"ğŸ“Š å½“å‰åˆ†å—prompt tokenä¼°ç®—: {prompt_tokens}")
        
        response = self.client.chat(
            model=self.model,
            messages=messages,
            temperature=0.7
        )
        return response.choices[0].message.content.strip()
        
    @retry_on_rate_limit(max_retries=3, delay=30.0, backoff_factor=1.5)
    def summarize(self, source: GPTSource) -> str:
        self.screenshot = source.screenshot
        self.link = source.link
        source.segment = self.ensure_segments_type(source.segment)
        
        # é¦–å…ˆä¼°ç®—æ€»tokenæ•°
        full_segment_text = self._build_segment_text(source.segment)
        estimated_tokens = estimate_tokens(full_segment_text)
        
        logger.info(f"ğŸ“Š è½¬å½•å†…å®¹tokenä¼°ç®—: {estimated_tokens}")
        
        # è®¾ç½®tokené™åˆ¶ï¼ˆæ ¹æ®ä¸åŒæ¨¡å‹è°ƒæ•´ï¼‰
        max_tokens = 80000  # é»˜è®¤é™åˆ¶
        if 'gpt-4' in self.model.lower():
            max_tokens = 120000  # GPT-4æœ‰æ›´é«˜çš„é™åˆ¶
        elif 'gpt-3.5' in self.model.lower():
            max_tokens = 14000   # GPT-3.5é™åˆ¶æ›´ä½
        
        logger.info(f"ğŸ“Š æ¨¡å‹ {self.model} çš„tokené™åˆ¶: {max_tokens}")
        
        # å¦‚æœå†…å®¹åœ¨é™åˆ¶èŒƒå›´å†…ï¼Œç›´æ¥å¤„ç†
        if estimated_tokens <= max_tokens - 10000:  # å¢åŠ é¢„ç•™tokenç©ºé—´åˆ°10000
            logger.info("ğŸ“ å†…å®¹æœªè¶…å‡ºé™åˆ¶ï¼Œç›´æ¥å¤„ç†")
            messages = self.create_messages(source.segment, source.title, source.tags)
            response = self.client.chat(
                model=self.model,
                messages=messages,
                temperature=0.7
            )
            return response.choices[0].message.content.strip()
        
        # å†…å®¹è¿‡é•¿ï¼Œéœ€è¦åˆ†å—å¤„ç†
        logger.warning(f"âš ï¸ å†…å®¹è¿‡é•¿ ({estimated_tokens} tokens)ï¼Œå¯ç”¨åˆ†å—å¤„ç†æ¨¡å¼")
        
        # åˆ†å‰²segments
        segment_chunks = split_segments_by_tokens(source.segment, max_tokens)
        logger.info(f"ğŸ”„ å·²åˆ†å‰²ä¸º {len(segment_chunks)} ä¸ªåˆ†å—")
        
        # å¤„ç†æ¯ä¸ªåˆ†å—
        chunk_results = []
        for i, chunk_segments in enumerate(segment_chunks):
            chunk_index = i + 1
            is_first = (i == 0)
            is_last = (i == len(segment_chunks) - 1)
            
            logger.info(f"ğŸ”„ å¤„ç†åˆ†å— {chunk_index}/{len(segment_chunks)}: {len(chunk_segments)} ä¸ªç‰‡æ®µ")
            
            # ä¸ºå½“å‰åˆ†å—åˆ›å»ºç‰¹æ®Šçš„prompt
            chunk_prompt = create_chunk_summary_prompt(
                chunk_index=chunk_index,
                total_chunks=len(segment_chunks),
                is_first=is_first,
                is_last=is_last
            )
            
            try:
                chunk_result = self._summarize_chunk(
                    chunk_segments,
                    source.title,
                    source.tags,
                    chunk_prompt
                )
                
                chunk_results.append(chunk_result)
                logger.info(f"âœ… åˆ†å— {chunk_index}/{len(segment_chunks)} å¤„ç†å®Œæˆ")
                
            except Exception as e:
                logger.error(f"âŒ åˆ†å— {chunk_index} å¤„ç†å¤±è´¥: {e}")
                # å¦‚æœæŸä¸ªåˆ†å—å¤±è´¥ï¼Œæ·»åŠ é”™è¯¯ä¿¡æ¯
                chunk_results.append(f"## ç¬¬ {chunk_index} éƒ¨åˆ†\n\n*æ­¤éƒ¨åˆ†å¤„ç†å¤±è´¥: {str(e)}*\n")
        
        # åˆå¹¶æ‰€æœ‰åˆ†å—ç»“æœ
        logger.info(f"ğŸ”— å¼€å§‹åˆå¹¶ {len(chunk_results)} ä¸ªåˆ†å—ç»“æœ")
        final_result = merge_markdown_contents(chunk_results)
        
        logger.info(f"âœ… åˆ†å—å¤„ç†å’Œåˆå¹¶å®Œæˆï¼Œæœ€ç»ˆå†…å®¹é•¿åº¦: {len(final_result)} å­—ç¬¦")
        return final_result

if __name__ == '__main__':
    gpt = OpenaiGPT()
    print(gpt.list_models())
