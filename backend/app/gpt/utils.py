import re
import math
from typing import List, Tuple
from app.models.transcriber_model import TranscriptSegment
from app.utils.logger import get_logger

logger = get_logger(__name__)

def fix_markdown(content: str) -> str:
    """ä¿®å¤markdownæ ¼å¼çš„å‡½æ•°ï¼ˆåŸæœ‰åŠŸèƒ½ä¿æŒï¼‰"""
    if not content:
        return content
    
    # ä¿®å¤å¯èƒ½çš„æ ¼å¼é—®é¢˜
    content = re.sub(r'^```markdown\s*\n', '', content, flags=re.MULTILINE)
    content = re.sub(r'\n```\s*$', '', content, flags=re.MULTILINE)
    
    return content.strip()


def estimate_tokens(text: str) -> int:
    """
    æ›´å‡†ç¡®åœ°ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡
    åŸºäºå®é™…æµ‹è¯•è°ƒæ•´çš„ä¼°ç®—å…¬å¼ï¼Œæ›´æ¥è¿‘çœŸå®tokenæ•°é‡
    """
    if not text:
        return 0
    
    # è®¡ç®—æ€»å­—ç¬¦æ•°
    total_chars = len(text)
    
    # è®¡ç®—ä¸­æ–‡å­—ç¬¦æ•°é‡
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    
    # è®¡ç®—è‹±æ–‡å•è¯æ•°é‡  
    english_words = len(re.findall(r'\b[a-zA-Z]+\b', text))
    
    # è®¡ç®—æ•°å­—ã€ç¬¦å·ã€æ ‡ç‚¹ç­‰
    other_chars = total_chars - chinese_chars - sum(len(word) for word in re.findall(r'\b[a-zA-Z]+\b', text))
    
    # æ›´ä¿å®ˆçš„ä¼°ç®—ï¼š
    # - ä¸­æ–‡å­—ç¬¦æŒ‰2.0ä¸ªtokenè®¡ç®—ï¼ˆä¹‹å‰1.5åå°ï¼‰
    # - è‹±æ–‡å•è¯æŒ‰1.3ä¸ªtokenè®¡ç®—ï¼ˆè€ƒè™‘å­è¯åˆ†å‰²ï¼‰
    # - å…¶ä»–å­—ç¬¦æŒ‰0.8ä¸ªtokenè®¡ç®—
    # - å†åŠ 20%çš„å®‰å…¨ä½™é‡
    estimated_tokens = int((chinese_chars * 2.0 + english_words * 1.3 + other_chars * 0.8) * 1.2)
    
    logger.debug(f"ğŸ“Š Tokenä¼°ç®—è¯¦æƒ…: æ€»å­—ç¬¦={total_chars}, ä¸­æ–‡={chinese_chars}, è‹±æ–‡å•è¯={english_words}, å…¶ä»–={other_chars}, ä¼°ç®—tokens={estimated_tokens}")
    
    return estimated_tokens


def estimate_image_tokens_from_base64(image_urls: list) -> int:
    """
    æ ¹æ®å›¾ç‰‡base64å­—ç¬¦ä¸²ä¼°ç®—tokenæ•°é‡
    base64ç¼–ç çš„å›¾ç‰‡åœ¨LLMä¸­é€šå¸¸æŒ‰å­—ç¬¦æ•°/4æ¥ä¼°ç®—tokenï¼ˆbase64ç¼–ç çº¦ä¸ºåŸå§‹æ•°æ®çš„4/3ï¼‰
    """
    if not image_urls:
        return 0
    
    total_tokens = 0
    for image_url in image_urls:
        if isinstance(image_url, str) and image_url.startswith('data:image/'):
            # æå–base64éƒ¨åˆ† (å»æ‰ "data:image/jpeg;base64," å‰ç¼€)
            base64_part = image_url.split(',', 1)[-1] if ',' in image_url else image_url
            
            # base64å­—ç¬¦æ•°é‡ä¼°ç®—tokenï¼ˆç»éªŒå…¬å¼ï¼šbase64å­—ç¬¦æ•° / 3ï¼‰
            base64_tokens = len(base64_part) // 3
            total_tokens += base64_tokens
            
            logger.debug(f"ğŸ“¸ å›¾ç‰‡tokenä¼°ç®—: base64é•¿åº¦={len(base64_part)}, ä¼°ç®—tokens={base64_tokens}")
        else:
            # ébase64å›¾ç‰‡ï¼ŒæŒ‰å›ºå®štokenä¼°ç®—
            total_tokens += 500
            logger.debug(f"ğŸ“¸ ébase64å›¾ç‰‡ï¼Œå›ºå®šä¼°ç®—500 tokens")
    
    logger.info(f"ğŸ“Š å›¾ç‰‡æ€»tokenä¼°ç®—: {len(image_urls)}å¼ å›¾ç‰‡, æ€»è®¡{total_tokens} tokens")
    return total_tokens


def estimate_mixed_content_tokens(text: str, image_urls: list = None) -> int:
    """
    ä¼°ç®—åŒ…å«æ–‡æœ¬å’Œå›¾ç‰‡çš„æ··åˆå†…å®¹tokenæ•°é‡
    """
    text_tokens = estimate_tokens(text) if text else 0
    image_tokens = estimate_image_tokens_from_base64(image_urls or [])
    total_tokens = text_tokens + image_tokens
    
    logger.info(f"ğŸ“Š æ··åˆå†…å®¹tokenä¼°ç®—: æ–‡æœ¬={text_tokens}, å›¾ç‰‡={image_tokens}, æ€»è®¡={total_tokens}")
    return total_tokens


def split_segments_with_images_by_tokens(
    segments: List[TranscriptSegment], 
    image_urls: List[str] = None,
    max_tokens: int = 80000
) -> List[Tuple[List[TranscriptSegment], List[str]]]:
    """
    æ ¹æ®tokené™åˆ¶å°†è½¬å½•ç‰‡æ®µå’Œå›¾ç‰‡åˆ†å‰²æˆå¤šä¸ªç»„ï¼Œç¡®ä¿å›¾ç‰‡ä¸è¢«æ‹†åˆ†
    
    Args:
        segments: è½¬å½•ç‰‡æ®µåˆ—è¡¨
        image_urls: å›¾ç‰‡URLåˆ—è¡¨
        max_tokens: æ¯ç»„çš„æœ€å¤§tokenæ•°
    
    Returns:
        åˆ†å‰²åçš„(ç‰‡æ®µç»„, å›¾ç‰‡ç»„)å…ƒç»„åˆ—è¡¨
    """
    if not segments:
        return []
    
    chunks = []
    current_chunk = []
    current_images = []
    current_tokens = 0
    
    # ä¸ºpromptæ¨¡æ¿é¢„ç•™tokenç©ºé—´
    template_reserve = 10000
    actual_max_tokens = max_tokens - template_reserve
    
    # è®¡ç®—å›¾ç‰‡æ€»tokenæ•°
    image_urls = image_urls or []
    total_image_tokens = estimate_image_tokens_from_base64(image_urls)
    
    logger.info(f"ğŸ“Š å¼€å§‹æ··åˆå†…å®¹åˆ†å‰²: è½¬å½•ç‰‡æ®µ={len(segments)}, å›¾ç‰‡={len(image_urls)}, å›¾ç‰‡tokens={total_image_tokens}")
    logger.info(f"ğŸ“Š æœ€å¤§tokenæ•°: {actual_max_tokens} (é¢„ç•™: {template_reserve})")
    
    # ç­–ç•¥1: å¦‚æœå›¾ç‰‡tokenæ•°é‡å¾ˆå¤§ï¼Œåªåœ¨ç¬¬ä¸€ä¸ªåˆ†å—ä¸­åŒ…å«å›¾ç‰‡
    if total_image_tokens > actual_max_tokens * 0.5:  # å›¾ç‰‡å ç”¨è¶…è¿‡50%
        logger.warning(f"âš ï¸ å›¾ç‰‡tokenå ç”¨è¿‡å¤§({total_image_tokens})ï¼Œåªåœ¨ç¬¬ä¸€ä¸ªåˆ†å—åŒ…å«å›¾ç‰‡")
        
        # ç¬¬ä¸€ä¸ªåˆ†å—ï¼šåŒ…å«å›¾ç‰‡å’Œéƒ¨åˆ†è½¬å½•
        first_chunk_max_tokens = actual_max_tokens - total_image_tokens
        first_chunk = []
        first_chunk_tokens = 0
        
        for segment in segments:
            segment_text = f"{format_time_from_seconds(segment.start)} - {segment.text.strip()}"
            segment_tokens = estimate_tokens(segment_text)
            
            if first_chunk_tokens + segment_tokens <= first_chunk_max_tokens:
                first_chunk.append(segment)
                first_chunk_tokens += segment_tokens
            else:
                break
        
        if first_chunk:
            chunks.append((first_chunk, image_urls))
            logger.info(f"ğŸ“¦ ç¬¬ä¸€åˆ†å—(å«å›¾ç‰‡): {len(first_chunk)}ä¸ªç‰‡æ®µ, {first_chunk_tokens + total_image_tokens} tokens")
            
            # å¤„ç†å‰©ä½™ç‰‡æ®µï¼ˆä¸åŒ…å«å›¾ç‰‡ï¼‰
            remaining_segments = segments[len(first_chunk):]
            if remaining_segments:
                remaining_chunks = split_segments_by_tokens(remaining_segments, max_tokens)
                for i, chunk_segments in enumerate(remaining_chunks):
                    chunks.append((chunk_segments, []))  # åç»­åˆ†å—ä¸åŒ…å«å›¾ç‰‡
                    chunk_tokens = sum(estimate_tokens(f"{format_time_from_seconds(seg.start)} - {seg.text.strip()}") 
                                     for seg in chunk_segments)
                    logger.info(f"ğŸ“¦ åç»­åˆ†å—{i+2}: {len(chunk_segments)}ä¸ªç‰‡æ®µ, {chunk_tokens} tokens")
        else:
            # å¦‚æœè¿ä¸€ä¸ªç‰‡æ®µéƒ½æ”¾ä¸ä¸‹ï¼Œå¼ºåˆ¶æ”¾å…¥ç¬¬ä¸€ä¸ªç‰‡æ®µ
            chunks.append(([segments[0]], image_urls))
            remaining_chunks = split_segments_by_tokens(segments[1:], max_tokens)
            for chunk_segments in remaining_chunks:
                chunks.append((chunk_segments, []))
                
        return chunks
    
    # ç­–ç•¥2: å›¾ç‰‡tokené€‚ä¸­ï¼Œå¯ä»¥åœ¨å¤šä¸ªåˆ†å—ä¸­åˆ†é…
    elif total_image_tokens <= actual_max_tokens * 0.3:  # å›¾ç‰‡å ç”¨ä¸è¶…è¿‡30%
        logger.info(f"âœ… å›¾ç‰‡tokené€‚ä¸­({total_image_tokens})ï¼Œåˆ†é…åˆ°å„ä¸ªåˆ†å—")
        
        # è®¡ç®—å¯ä»¥åˆ†é…å›¾ç‰‡çš„åˆ†å—æ•°é‡
        images_per_chunk = max(1, len(image_urls) // 3)  # å¹³å‡åˆ†é…ï¼Œä½†æ¯å—è‡³å°‘1å¼ 
        image_chunks = [image_urls[i:i + images_per_chunk] for i in range(0, len(image_urls), images_per_chunk)]
        
        current_image_idx = 0
        
        for i, segment in enumerate(segments):
            segment_text = f"{format_time_from_seconds(segment.start)} - {segment.text.strip()}"
            segment_tokens = estimate_tokens(segment_text)
            
            # è®¡ç®—å½“å‰åˆ†å—å¦‚æœåŠ ä¸Šå›¾ç‰‡çš„tokenæ•°
            current_image_chunk = image_chunks[current_image_idx] if current_image_idx < len(image_chunks) else []
            current_image_tokens = estimate_image_tokens_from_base64(current_image_chunk)
            
            # æ£€æŸ¥æ˜¯å¦è¶…å‡ºé™åˆ¶
            if current_tokens + segment_tokens + current_image_tokens > actual_max_tokens and current_chunk:
                # ä¿å­˜å½“å‰åˆ†å—
                chunks.append((current_chunk, current_images))
                logger.info(f"ğŸ“¦ å®Œæˆåˆ†å— {len(chunks)}: {len(current_chunk)}ä¸ªç‰‡æ®µ, {current_tokens}æ–‡æœ¬+{estimate_image_tokens_from_base64(current_images)}å›¾ç‰‡ tokens")
                
                # å¼€å§‹æ–°åˆ†å—
                current_chunk = [segment]
                current_tokens = segment_tokens
                current_image_idx = min(current_image_idx + 1, len(image_chunks) - 1)
                current_images = image_chunks[current_image_idx] if current_image_idx < len(image_chunks) else []
            else:
                current_chunk.append(segment)
                current_tokens += segment_tokens
                if not current_images and current_image_idx < len(image_chunks):
                    current_images = image_chunks[current_image_idx]
        
        # æ·»åŠ æœ€åä¸€ä¸ªç»„
        if current_chunk:
            chunks.append((current_chunk, current_images))
            logger.info(f"ğŸ“¦ å®Œæˆåˆ†å— {len(chunks)}: {len(current_chunk)}ä¸ªç‰‡æ®µ, {current_tokens}æ–‡æœ¬+{estimate_image_tokens_from_base64(current_images)}å›¾ç‰‡ tokens")
        
        return chunks
    
    # ç­–ç•¥3: å›¾ç‰‡tokenè¾ƒå¤§ä½†å¯ä»¥å¤„ç†ï¼Œåªåœ¨ç¬¬ä¸€ä¸ªåˆ†å—åŒ…å«
    else:
        logger.info(f"ğŸ“Š å›¾ç‰‡tokenè¾ƒå¤§({total_image_tokens})ï¼Œåªåœ¨ç¬¬ä¸€ä¸ªåˆ†å—åŒ…å«")
        return split_segments_with_images_by_tokens(segments, image_urls, max_tokens)


def split_segments_by_tokens(segments: List[TranscriptSegment], max_tokens: int = 80000) -> List[List[TranscriptSegment]]:
    """
    æ ¹æ®tokené™åˆ¶å°†è½¬å½•ç‰‡æ®µåˆ†å‰²æˆå¤šä¸ªç»„
    
    Args:
        segments: è½¬å½•ç‰‡æ®µåˆ—è¡¨
        max_tokens: æ¯ç»„çš„æœ€å¤§tokenæ•°
    
    Returns:
        åˆ†å‰²åçš„ç‰‡æ®µç»„åˆ—è¡¨
    """
    if not segments:
        return []
    
    chunks = []
    current_chunk = []
    current_tokens = 0
    
    # ä¸ºpromptæ¨¡æ¿é¢„ç•™tokenç©ºé—´
    template_reserve = 10000  # å¢åŠ é¢„ç•™ç©ºé—´ï¼Œä¸ºpromptæ¨¡æ¿ã€æ ‡é¢˜ã€æ ‡ç­¾ç­‰é¢„ç•™æ›´å¤štoken
    actual_max_tokens = max_tokens - template_reserve
    
    logger.info(f"ğŸ“Š å¼€å§‹åˆ†å‰²è½¬å½•ç‰‡æ®µï¼Œæœ€å¤§tokenæ•°: {actual_max_tokens} (é¢„ç•™: {template_reserve})")
    
    for i, segment in enumerate(segments):
        segment_text = f"{format_time_from_seconds(segment.start)} - {segment.text.strip()}"
        segment_tokens = estimate_tokens(segment_text)
        
        # å¦‚æœå½“å‰ç‰‡æ®µåŠ å…¥åä¼šè¶…å‡ºé™åˆ¶ï¼Œä¸”å½“å‰ç»„ä¸ä¸ºç©ºï¼Œåˆ™å¼€å§‹æ–°ç»„
        if current_tokens + segment_tokens > actual_max_tokens and current_chunk:
            logger.info(f"ğŸ“¦ å®Œæˆåˆ†å— {len(chunks) + 1}: {len(current_chunk)} ä¸ªç‰‡æ®µ, {current_tokens} tokens")
            chunks.append(current_chunk)
            current_chunk = [segment]
            current_tokens = segment_tokens
        else:
            current_chunk.append(segment)
            current_tokens += segment_tokens
    
    # æ·»åŠ æœ€åä¸€ä¸ªç»„
    if current_chunk:
        logger.info(f"ğŸ“¦ å®Œæˆåˆ†å— {len(chunks) + 1}: {len(current_chunk)} ä¸ªç‰‡æ®µ, {current_tokens} tokens")
        chunks.append(current_chunk)
    
    logger.info(f"âœ… åˆ†å‰²å®Œæˆï¼Œå…±ç”Ÿæˆ {len(chunks)} ä¸ªåˆ†å—")
    return chunks


def format_time_from_seconds(seconds: float) -> str:
    """å°†ç§’æ•°æ ¼å¼åŒ–ä¸ºæ—¶é—´å­—ç¬¦ä¸² HH:MM:SS"""
    from datetime import timedelta
    return str(timedelta(seconds=int(seconds)))[2:]  # å»æ‰å°æ—¶å‰ç¼€çš„0:


def merge_markdown_contents(contents: List[str]) -> str:
    """
    åˆå¹¶å¤šä¸ªmarkdownå†…å®¹ï¼Œæ™ºèƒ½å¤„ç†é‡å¤çš„æ ‡é¢˜å’Œå†…å®¹
    
    Args:
        contents: markdownå†…å®¹åˆ—è¡¨
    
    Returns:
        åˆå¹¶åçš„markdownå†…å®¹
    """
    if not contents:
        return ""
    
    if len(contents) == 1:
        return contents[0]
    
    logger.info(f"ğŸ”— å¼€å§‹åˆå¹¶ {len(contents)} ä¸ªmarkdownå†…å®¹")
    
    merged_content = []
    section_counter = 1
    
    # æå–ç¬¬ä¸€ä¸ªå†…å®¹çš„æ ‡é¢˜éƒ¨åˆ†ï¼ˆé€šå¸¸åŒ…å«è§†é¢‘æ ‡é¢˜ç­‰ä¿¡æ¯ï¼‰
    first_content = contents[0].strip()
    
    # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªäºŒçº§æ ‡é¢˜çš„ä½ç½®ï¼Œä¹‹å‰çš„å†…å®¹ä½œä¸ºå¤´éƒ¨
    header_match = re.search(r'\n## ', first_content)
    if header_match:
        header = first_content[:header_match.start()].strip()
        merged_content.append(header)
        merged_content.append("\n")
    
    # å¤„ç†æ¯ä¸ªåˆ†å—çš„å†…å®¹
    for i, content in enumerate(contents):
        content = content.strip()
        
        # è·³è¿‡ç©ºå†…å®¹
        if not content:
            continue
        
        # ç§»é™¤ç¬¬äºŒä¸ªåŠåç»­å†…å®¹çš„å¤´éƒ¨ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€æ ‡ç­¾ç­‰ï¼‰
        if i > 0:
            # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªäºŒçº§æ ‡é¢˜ï¼Œä»é‚£é‡Œå¼€å§‹æå–å†…å®¹
            header_match = re.search(r'\n## ', content)
            if header_match:
                content = content[header_match.start():].strip()
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°äºŒçº§æ ‡é¢˜ï¼ŒæŸ¥æ‰¾ç¬¬ä¸€ä¸ªæœ‰æ„ä¹‰çš„å†…å®¹è¡Œ
                lines = content.split('\n')
                content_start = 0
                for j, line in enumerate(lines):
                    if line.strip() and not line.startswith('#') and not line.startswith('è§†é¢‘æ ‡é¢˜') and not line.startswith('è§†é¢‘æ ‡ç­¾'):
                        content_start = j
                        break
                content = '\n'.join(lines[content_start:]).strip()
        
        # ä¸ºæ¯ä¸ªåˆ†å—æ·»åŠ åˆ†èŠ‚æ ‡è¯†
        if i > 0:  # ç¬¬ä¸€ä¸ªåˆ†å—ä¸éœ€è¦é¢å¤–æ ‡è¯†
            merged_content.append(f"\n\n## ç¬¬ {section_counter} éƒ¨åˆ†ï¼ˆç»­ï¼‰\n")
            section_counter += 1
        
        # æ·»åŠ å†…å®¹ï¼Œä½†ç§»é™¤å¼€å¤´çš„æ ‡é¢˜ä¿¡æ¯
        if i == 0:
            # ç¬¬ä¸€ä¸ªå†…å®¹ï¼ŒæŸ¥æ‰¾ç¬¬ä¸€ä¸ªäºŒçº§æ ‡é¢˜å¼€å§‹çš„åœ°æ–¹
            header_match = re.search(r'\n## ', content)
            if header_match:
                main_content = content[header_match.start():].strip()
                merged_content.append(main_content)
            else:
                merged_content.append(content)
        else:
            merged_content.append(content)
    
    # æ·»åŠ åˆå¹¶è¯´æ˜
    merged_content.append(f"\n\n---\n\n## ğŸ“‹ ç”Ÿæˆè¯´æ˜\n\næœ¬ç¬”è®°ç”±äºå†…å®¹è¾ƒé•¿ï¼Œé‡‡ç”¨äº†åˆ†å—å¤„ç†å¹¶åˆå¹¶ç”Ÿæˆã€‚å…±å¤„ç†äº† {len(contents)} ä¸ªå†…å®¹åˆ†å—ã€‚")
    
    result = '\n'.join(merged_content)
    logger.info(f"âœ… å†…å®¹åˆå¹¶å®Œæˆï¼Œæœ€ç»ˆé•¿åº¦: {len(result)} å­—ç¬¦")
    
    return result


def create_chunk_summary_prompt(chunk_index: int, total_chunks: int, is_first: bool = False, is_last: bool = False) -> str:
    """
    ä¸ºåˆ†å—å¤„ç†åˆ›å»ºç‰¹æ®Šçš„promptè¯´æ˜
    
    Args:
        chunk_index: å½“å‰åˆ†å—ç´¢å¼•ï¼ˆä»1å¼€å§‹ï¼‰
        total_chunks: æ€»åˆ†å—æ•°
        is_first: æ˜¯å¦ä¸ºç¬¬ä¸€ä¸ªåˆ†å—
        is_last: æ˜¯å¦ä¸ºæœ€åä¸€ä¸ªåˆ†å—
    
    Returns:
        åˆ†å—å¤„ç†çš„promptè¯´æ˜
    """
    prompt_parts = []
    
    if is_first:
        prompt_parts.append(f"""
ğŸ“‹ **å†…å®¹åˆ†å—å¤„ç†è¯´æ˜**ï¼š
ç”±äºè§†é¢‘å†…å®¹è¾ƒé•¿ï¼Œæœ¬æ¬¡å¤„ç†é‡‡ç”¨åˆ†å—æ¨¡å¼ã€‚è¿™æ˜¯ç¬¬ {chunk_index}/{total_chunks} éƒ¨åˆ†ã€‚

å¯¹äºç¬¬ä¸€éƒ¨åˆ†ï¼Œè¯·ï¼š
1. ç”Ÿæˆå®Œæ•´çš„ç¬”è®°å¼€å¤´ï¼ˆåŒ…æ‹¬æ ‡é¢˜ã€æ¦‚è¿°ç­‰ï¼‰
2. å¤„ç†è¿™éƒ¨åˆ†çš„è¯¦ç»†å†…å®¹
3. ä¿æŒæ­£å¸¸çš„markdownç»“æ„
""")
    elif is_last:
        prompt_parts.append(f"""
ğŸ“‹ **å†…å®¹åˆ†å—å¤„ç†è¯´æ˜**ï¼š
è¿™æ˜¯ç¬¬ {chunk_index}/{total_chunks} éƒ¨åˆ†ï¼ˆæœ€åä¸€éƒ¨åˆ†ï¼‰ã€‚

å¯¹äºæœ€åéƒ¨åˆ†ï¼Œè¯·ï¼š
1. å¤„ç†è¿™éƒ¨åˆ†çš„è¯¦ç»†å†…å®¹
2. åœ¨æœ«å°¾ç”Ÿæˆå®Œæ•´çš„æ€»ç»“
3. ä¸éœ€è¦é‡å¤æ ‡é¢˜ä¿¡æ¯
""")
    else:
        prompt_parts.append(f"""
ğŸ“‹ **å†…å®¹åˆ†å—å¤„ç†è¯´æ˜**ï¼š
è¿™æ˜¯ç¬¬ {chunk_index}/{total_chunks} éƒ¨åˆ†ï¼ˆä¸­é—´éƒ¨åˆ†ï¼‰ã€‚

å¯¹äºä¸­é—´éƒ¨åˆ†ï¼Œè¯·ï¼š
1. ç›´æ¥å¤„ç†å†…å®¹ï¼Œä¸éœ€è¦é‡å¤æ ‡é¢˜
2. ä¿æŒä¸å‰åéƒ¨åˆ†çš„è¿è´¯æ€§
3. é‡ç‚¹å…³æ³¨è¿™éƒ¨åˆ†çš„æ ¸å¿ƒå†…å®¹
""")
    
    return '\n'.join(prompt_parts)