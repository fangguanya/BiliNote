import re
import math
from typing import List, Tuple
from app.models.transcriber_model import TranscriptSegment
from app.utils.logger import get_logger

logger = get_logger(__name__)

def fix_markdown(content: str) -> str:
    """ä¿®å¤markdownæ ¼å¼çš„å‡½æ•°ï¼ˆåŸæœ‰åŠŸèƒ½ä¿æŒï¼‰"""
    if not content:
        return content
    
    # ä¿®å¤å¯èƒ½çš„æ ¼å¼é—®é¢˜
    content = re.sub(r'^```markdown\s*\n', '', content, flags=re.MULTILINE)
    content = re.sub(r'\n```\s*$', '', content, flags=re.MULTILINE)
    
    return content.strip()


def estimate_tokens(text: str) -> int:
    """
    æ›´å‡†ç¡®åœ°ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡
    åŸºäºå®é™…æµ‹è¯•è°ƒæ•´çš„ä¼°ç®—å…¬å¼ï¼Œæ›´æ¥è¿‘çœŸå®tokenæ•°é‡
    """
    if not text:
        return 0
    
    # è®¡ç®—æ€»å­—ç¬¦æ•°
    total_chars = len(text)
    
    # è®¡ç®—ä¸­æ–‡å­—ç¬¦æ•°é‡
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    
    # è®¡ç®—è‹±æ–‡å•è¯æ•°é‡  
    english_words = len(re.findall(r'\b[a-zA-Z]+\b', text))
    
    # è®¡ç®—æ•°å­—ã€ç¬¦å·ã€æ ‡ç‚¹ç­‰
    other_chars = total_chars - chinese_chars - sum(len(word) for word in re.findall(r'\b[a-zA-Z]+\b', text))
    
    # æ›´ä¿å®ˆçš„ä¼°ç®—ï¼š
    # - ä¸­æ–‡å­—ç¬¦æŒ‰2.0ä¸ªtokenè®¡ç®—ï¼ˆä¹‹å‰1.5åå°ï¼‰
    # - è‹±æ–‡å•è¯æŒ‰1.3ä¸ªtokenè®¡ç®—ï¼ˆè€ƒè™‘å­è¯åˆ†å‰²ï¼‰
    # - å…¶ä»–å­—ç¬¦æŒ‰0.8ä¸ªtokenè®¡ç®—
    # - å†åŠ 20%çš„å®‰å…¨ä½™é‡
    estimated_tokens = int((chinese_chars * 2.0 + english_words * 1.3 + other_chars * 0.8) * 1.2)
    
    logger.debug(f"ğŸ“Š Tokenä¼°ç®—è¯¦æƒ…: æ€»å­—ç¬¦={total_chars}, ä¸­æ–‡={chinese_chars}, è‹±æ–‡å•è¯={english_words}, å…¶ä»–={other_chars}, ä¼°ç®—tokens={estimated_tokens}")
    
    return estimated_tokens


def estimate_image_tokens_from_base64(image_urls: list) -> int:
    """
    æ ¹æ®å›¾ç‰‡base64å­—ç¬¦ä¸²ä¼°ç®—tokenæ•°é‡
    base64ç¼–ç çš„å›¾ç‰‡åœ¨LLMä¸­é€šå¸¸æŒ‰å­—ç¬¦æ•°/4æ¥ä¼°ç®—tokenï¼ˆbase64ç¼–ç çº¦ä¸ºåŸå§‹æ•°æ®çš„4/3ï¼‰
    """
    if not image_urls:
        return 0
    
    total_tokens = 0
    for image_url in image_urls:
        if isinstance(image_url, str) and image_url.startswith('data:image/'):
            # æå–base64éƒ¨åˆ† (å»æ‰ "data:image/jpeg;base64," å‰ç¼€)
            base64_part = image_url.split(',', 1)[-1] if ',' in image_url else image_url
            
            # base64å­—ç¬¦æ•°é‡ä¼°ç®—tokenï¼ˆç»éªŒå…¬å¼ï¼šbase64å­—ç¬¦æ•° / 3ï¼‰
            base64_tokens = len(base64_part) // 15
            total_tokens += base64_tokens
            
            logger.debug(f"ğŸ“¸ å›¾ç‰‡tokenä¼°ç®—: base64é•¿åº¦={len(base64_part)}, ä¼°ç®—tokens={base64_tokens}")
        else:
            # ébase64å›¾ç‰‡ï¼ŒæŒ‰å›ºå®štokenä¼°ç®—
            total_tokens += 500
            logger.debug(f"ğŸ“¸ ébase64å›¾ç‰‡ï¼Œå›ºå®šä¼°ç®—500 tokens")
    
    logger.info(f"ğŸ“Š å›¾ç‰‡æ€»tokenä¼°ç®—: {len(image_urls)}å¼ å›¾ç‰‡, æ€»è®¡{total_tokens} tokens")
    return total_tokens


def estimate_mixed_content_tokens(text: str, image_urls: list = None) -> int:
    """
    ä¼°ç®—åŒ…å«æ–‡æœ¬å’Œå›¾ç‰‡çš„æ··åˆå†…å®¹tokenæ•°é‡
    """
    text_tokens = estimate_tokens(text) if text else 0
    image_tokens = estimate_image_tokens_from_base64(image_urls or [])
    total_tokens = text_tokens + image_tokens
    
    logger.info(f"ğŸ“Š æ··åˆå†…å®¹tokenä¼°ç®—: æ–‡æœ¬={text_tokens}, å›¾ç‰‡={image_tokens}, æ€»è®¡={total_tokens}")
    return total_tokens


def split_segments_with_images_by_tokens(
    segments: List[TranscriptSegment], 
    image_urls: List[str] = None,
    max_tokens: int = 80000
) -> List[Tuple[List[TranscriptSegment], List[str]]]:
    """
    æ ¹æ®tokené™åˆ¶å°†è½¬å½•ç‰‡æ®µå’Œå›¾ç‰‡åˆ†å‰²æˆå¤šä¸ªç»„ï¼Œå›¾ç‰‡å¯ä»¥åˆ†æ•£åˆ°ä¸åŒåˆ†å—
    
    Args:
        segments: è½¬å½•ç‰‡æ®µåˆ—è¡¨
        image_urls: å›¾ç‰‡URLåˆ—è¡¨
        max_tokens: æ¯ç»„çš„æœ€å¤§tokenæ•°
    
    Returns:
        åˆ†å‰²åçš„(ç‰‡æ®µç»„, å›¾ç‰‡ç»„)å…ƒç»„åˆ—è¡¨
    """
    if not segments:
        return []
    
    chunks = []
    current_chunk = []
    current_images = []
    current_tokens = 0
    
    # ä¸ºpromptæ¨¡æ¿é¢„ç•™tokenç©ºé—´
    template_reserve = 10000
    actual_max_tokens = max_tokens - template_reserve
    
    # è®¡ç®—å›¾ç‰‡tokenä¿¡æ¯
    image_urls = image_urls or []
    image_tokens_list = []  # æ¯å¼ å›¾ç‰‡çš„tokenæ•°
    total_image_tokens = 0
    
    for image_url in image_urls:
        tokens = estimate_image_tokens_from_base64([image_url])
        image_tokens_list.append(tokens)
        total_image_tokens += tokens
    
    logger.info(f"ğŸ“Š å¼€å§‹æ··åˆå†…å®¹åˆ†å‰²: è½¬å½•ç‰‡æ®µ={len(segments)}, å›¾ç‰‡={len(image_urls)}, å›¾ç‰‡æ€»tokens={total_image_tokens}")
    logger.info(f"ğŸ“Š æœ€å¤§tokenæ•°: {actual_max_tokens} (é¢„ç•™: {template_reserve})")
    
    # å¦‚æœæ²¡æœ‰å›¾ç‰‡ï¼Œä½¿ç”¨åŸæœ‰çš„åˆ†å—é€»è¾‘
    if not image_urls:
        segment_chunks = split_segments_by_tokens(segments, max_tokens)
        return [(chunk, []) for chunk in segment_chunks]
    
    # åˆ›å»ºå›¾ç‰‡é˜Ÿåˆ—ï¼Œç”¨äºåˆ†é…
    available_images = list(zip(image_urls, image_tokens_list))
    image_queue_index = 0
    
    for i, segment in enumerate(segments):
        segment_text = f"{format_time_from_seconds(segment.start)} - {segment.text.strip()}"
        segment_tokens = estimate_tokens(segment_text)
        
        # å°è¯•ä»å›¾ç‰‡é˜Ÿåˆ—ä¸­æ·»åŠ å›¾ç‰‡åˆ°å½“å‰åˆ†å—
        while image_queue_index < len(available_images):
            next_image_url, next_image_tokens = available_images[image_queue_index]
            
            # æ£€æŸ¥å½“å‰åˆ†å—æ˜¯å¦èƒ½å®¹çº³è¿™å¼ å›¾ç‰‡
            potential_tokens = current_tokens + segment_tokens + next_image_tokens
            current_image_tokens = sum(tokens for _, tokens in current_images)
            potential_total = current_tokens + segment_tokens + current_image_tokens + next_image_tokens
            
            if potential_total <= actual_max_tokens:
                # å¯ä»¥æ·»åŠ è¿™å¼ å›¾ç‰‡
                current_images.append((next_image_url, next_image_tokens))
                image_queue_index += 1
                logger.debug(f"ğŸ“¸ æ·»åŠ å›¾ç‰‡åˆ°åˆ†å— {len(chunks)+1}: {next_image_tokens} tokens")
            else:
                # å½“å‰åˆ†å—æ— æ³•å®¹çº³æ›´å¤šå›¾ç‰‡ï¼Œåœæ­¢æ·»åŠ 
                break
        
        # æ£€æŸ¥å½“å‰ç‰‡æ®µæ˜¯å¦å¯ä»¥æ·»åŠ åˆ°å½“å‰åˆ†å—
        current_image_tokens = sum(tokens for _, tokens in current_images)
        total_current_tokens = current_tokens + segment_tokens + current_image_tokens
        
        if total_current_tokens > actual_max_tokens and current_chunk:
            # å½“å‰åˆ†å—å·²æ»¡ï¼Œä¿å­˜å¹¶å¼€å§‹æ–°åˆ†å—
            final_images = [img_url for img_url, _ in current_images]
            chunks.append((current_chunk, final_images))
            
            total_tokens = current_tokens + current_image_tokens
            logger.info(f"ğŸ“¦ å®Œæˆåˆ†å— {len(chunks)}: {len(current_chunk)}ä¸ªç‰‡æ®µ, {len(final_images)}å¼ å›¾ç‰‡, {current_tokens}æ–‡æœ¬+{current_image_tokens}å›¾ç‰‡={total_tokens} tokens")
            
            # å¼€å§‹æ–°åˆ†å—
            current_chunk = [segment]
            current_images = []
            current_tokens = segment_tokens
        else:
            # æ·»åŠ ç‰‡æ®µåˆ°å½“å‰åˆ†å—
            current_chunk.append(segment)
            current_tokens += segment_tokens
    
    # å¦‚æœè¿˜æœ‰å‰©ä½™çš„å›¾ç‰‡æœªåˆ†é…ï¼Œå°è¯•åˆ†é…åˆ°æœ€åä¸€ä¸ªåˆ†å—æˆ–åˆ›å»ºæ–°åˆ†å—
    while image_queue_index < len(available_images):
        remaining_images = available_images[image_queue_index:]
        remaining_image_tokens = sum(tokens for _, tokens in remaining_images)
        
        current_image_tokens = sum(tokens for _, tokens in current_images)
        total_current_tokens = current_tokens + current_image_tokens + remaining_image_tokens
        
        if total_current_tokens <= actual_max_tokens:
            # æ‰€æœ‰å‰©ä½™å›¾ç‰‡éƒ½å¯ä»¥åŠ å…¥å½“å‰åˆ†å—
            current_images.extend(remaining_images)
            image_queue_index = len(available_images)
            logger.info(f"ğŸ“¸ å°†å‰©ä½™ {len(remaining_images)} å¼ å›¾ç‰‡æ·»åŠ åˆ°æœ€ååˆ†å—")
        else:
            # éœ€è¦ä¸ºå‰©ä½™å›¾ç‰‡åˆ›å»ºæ–°åˆ†å—ï¼Œå°è¯•é€å¼ æ·»åŠ 
            while image_queue_index < len(available_images):
                next_image_url, next_image_tokens = available_images[image_queue_index]
                current_image_tokens = sum(tokens for _, tokens in current_images)
                
                if current_tokens + current_image_tokens + next_image_tokens <= actual_max_tokens:
                    current_images.append((next_image_url, next_image_tokens))
                    image_queue_index += 1
                else:
                    # å½“å‰åˆ†å—æ— æ³•å®¹çº³ï¼Œç»“æŸå½“å‰åˆ†å—ï¼Œä¸ºå‰©ä½™å›¾ç‰‡åˆ›å»ºæ–°åˆ†å—
                    break
            
            # å¦‚æœè¿˜æœ‰æœªåˆ†é…çš„å›¾ç‰‡ï¼Œä¸ºå®ƒä»¬åˆ›å»ºæ–°çš„åˆ†å—
            if image_queue_index < len(available_images):
                # ä¿å­˜å½“å‰åˆ†å—
                if current_chunk:
                    final_images = [img_url for img_url, _ in current_images]
                    chunks.append((current_chunk, final_images))
                    
                    current_image_tokens = sum(tokens for _, tokens in current_images)
                    total_tokens = current_tokens + current_image_tokens
                    logger.info(f"ğŸ“¦ å®Œæˆåˆ†å— {len(chunks)}: {len(current_chunk)}ä¸ªç‰‡æ®µ, {len(final_images)}å¼ å›¾ç‰‡, {current_tokens}æ–‡æœ¬+{current_image_tokens}å›¾ç‰‡={total_tokens} tokens")
                
                # ä¸ºå‰©ä½™å›¾ç‰‡åˆ›å»ºæ–°åˆ†å—ï¼ˆåªåŒ…å«å›¾ç‰‡ï¼Œä¸åŒ…å«è½¬å½•ï¼‰
                remaining_images = available_images[image_queue_index:]
                remaining_chunk_images = []
                remaining_tokens = 0
                
                for img_url, img_tokens in remaining_images:
                    if remaining_tokens + img_tokens <= actual_max_tokens:
                        remaining_chunk_images.append(img_url)
                        remaining_tokens += img_tokens
                    else:
                        # å¦‚æœè¿˜æœ‰å›¾ç‰‡è£…ä¸ä¸‹ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†å—
                        if remaining_chunk_images:
                            chunks.append(([], remaining_chunk_images))
                            logger.info(f"ğŸ“¦ å®Œæˆå›¾ç‰‡åˆ†å— {len(chunks)}: 0ä¸ªç‰‡æ®µ, {len(remaining_chunk_images)}å¼ å›¾ç‰‡, {remaining_tokens} tokens")
                        
                        remaining_chunk_images = [img_url]
                        remaining_tokens = img_tokens
                
                # æ·»åŠ æœ€åçš„å›¾ç‰‡åˆ†å—
                if remaining_chunk_images:
                    chunks.append(([], remaining_chunk_images))
                    logger.info(f"ğŸ“¦ å®Œæˆæœ€åå›¾ç‰‡åˆ†å— {len(chunks)}: 0ä¸ªç‰‡æ®µ, {len(remaining_chunk_images)}å¼ å›¾ç‰‡, {remaining_tokens} tokens")
                
                image_queue_index = len(available_images)
                current_chunk = []
                current_images = []
                current_tokens = 0
    
    # æ·»åŠ æœ€åä¸€ä¸ªåˆ†å—ï¼ˆå¦‚æœæœ‰å†…å®¹ï¼‰
    if current_chunk or current_images:
        final_images = [img_url for img_url, _ in current_images]
        chunks.append((current_chunk, final_images))
        
        current_image_tokens = sum(tokens for _, tokens in current_images)
        total_tokens = current_tokens + current_image_tokens
        logger.info(f"ğŸ“¦ å®Œæˆæœ€ååˆ†å— {len(chunks)}: {len(current_chunk)}ä¸ªç‰‡æ®µ, {len(final_images)}å¼ å›¾ç‰‡, {current_tokens}æ–‡æœ¬+{current_image_tokens}å›¾ç‰‡={total_tokens} tokens")
    
    logger.info(f"âœ… åˆ†å—å®Œæˆï¼Œå…± {len(chunks)} ä¸ªåˆ†å—ï¼Œå›¾ç‰‡åˆ†é…æƒ…å†µ:")
    for i, (chunk_segments, chunk_images) in enumerate(chunks):
        logger.info(f"   åˆ†å—{i+1}: {len(chunk_segments)}ä¸ªç‰‡æ®µ, {len(chunk_images)}å¼ å›¾ç‰‡")
    
    return chunks


def split_segments_by_tokens(segments: List[TranscriptSegment], max_tokens: int = 80000) -> List[List[TranscriptSegment]]:
    """
    æ ¹æ®tokené™åˆ¶å°†è½¬å½•ç‰‡æ®µåˆ†å‰²æˆå¤šä¸ªç»„
    
    Args:
        segments: è½¬å½•ç‰‡æ®µåˆ—è¡¨
        max_tokens: æ¯ç»„çš„æœ€å¤§tokenæ•°
    
    Returns:
        åˆ†å‰²åçš„ç‰‡æ®µç»„åˆ—è¡¨
    """
    if not segments:
        return []
    
    chunks = []
    current_chunk = []
    current_tokens = 0
    
    # ä¸ºpromptæ¨¡æ¿é¢„ç•™tokenç©ºé—´
    template_reserve = 10000  # å¢åŠ é¢„ç•™ç©ºé—´ï¼Œä¸ºpromptæ¨¡æ¿ã€æ ‡é¢˜ã€æ ‡ç­¾ç­‰é¢„ç•™æ›´å¤štoken
    actual_max_tokens = max_tokens - template_reserve
    
    logger.info(f"ğŸ“Š å¼€å§‹åˆ†å‰²è½¬å½•ç‰‡æ®µï¼Œæœ€å¤§tokenæ•°: {actual_max_tokens} (é¢„ç•™: {template_reserve})")
    
    for i, segment in enumerate(segments):
        segment_text = f"{format_time_from_seconds(segment.start)} - {segment.text.strip()}"
        segment_tokens = estimate_tokens(segment_text)
        
        # å¦‚æœå½“å‰ç‰‡æ®µåŠ å…¥åä¼šè¶…å‡ºé™åˆ¶ï¼Œä¸”å½“å‰ç»„ä¸ä¸ºç©ºï¼Œåˆ™å¼€å§‹æ–°ç»„
        if current_tokens + segment_tokens > actual_max_tokens and current_chunk:
            logger.info(f"ğŸ“¦ å®Œæˆåˆ†å— {len(chunks) + 1}: {len(current_chunk)} ä¸ªç‰‡æ®µ, {current_tokens} tokens")
            chunks.append(current_chunk)
            current_chunk = [segment]
            current_tokens = segment_tokens
        else:
            current_chunk.append(segment)
            current_tokens += segment_tokens
    
    # æ·»åŠ æœ€åä¸€ä¸ªç»„
    if current_chunk:
        logger.info(f"ğŸ“¦ å®Œæˆåˆ†å— {len(chunks) + 1}: {len(current_chunk)} ä¸ªç‰‡æ®µ, {current_tokens} tokens")
        chunks.append(current_chunk)
    
    logger.info(f"âœ… åˆ†å‰²å®Œæˆï¼Œå…±ç”Ÿæˆ {len(chunks)} ä¸ªåˆ†å—")
    return chunks


def format_time_from_seconds(seconds: float) -> str:
    """å°†ç§’æ•°æ ¼å¼åŒ–ä¸ºæ—¶é—´å­—ç¬¦ä¸² HH:MM:SS"""
    from datetime import timedelta
    return str(timedelta(seconds=int(seconds)))[2:]  # å»æ‰å°æ—¶å‰ç¼€çš„0:


def merge_markdown_contents(contents: List[str]) -> str:
    """
    åˆå¹¶å¤šä¸ªmarkdownå†…å®¹ï¼Œæ™ºèƒ½å¤„ç†é‡å¤çš„æ ‡é¢˜å’Œå†…å®¹
    
    Args:
        contents: markdownå†…å®¹åˆ—è¡¨
    
    Returns:
        åˆå¹¶åçš„markdownå†…å®¹
    """
    if not contents:
        return ""
    
    if len(contents) == 1:
        return contents[0]
    
    logger.info(f"ğŸ”— å¼€å§‹åˆå¹¶ {len(contents)} ä¸ªmarkdownå†…å®¹")
    
    merged_content = []
    section_counter = 1
    
    # æå–ç¬¬ä¸€ä¸ªå†…å®¹çš„æ ‡é¢˜éƒ¨åˆ†ï¼ˆé€šå¸¸åŒ…å«è§†é¢‘æ ‡é¢˜ç­‰ä¿¡æ¯ï¼‰
    first_content = contents[0].strip()
    
    # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªäºŒçº§æ ‡é¢˜çš„ä½ç½®ï¼Œä¹‹å‰çš„å†…å®¹ä½œä¸ºå¤´éƒ¨
    header_match = re.search(r'\n## ', first_content)
    if header_match:
        header = first_content[:header_match.start()].strip()
        merged_content.append(header)
        merged_content.append("\n")
    
    # å¤„ç†æ¯ä¸ªåˆ†å—çš„å†…å®¹
    for i, content in enumerate(contents):
        content = content.strip()
        
        # è·³è¿‡ç©ºå†…å®¹
        if not content:
            continue
        
        # ç§»é™¤ç¬¬äºŒä¸ªåŠåç»­å†…å®¹çš„å¤´éƒ¨ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€æ ‡ç­¾ç­‰ï¼‰
        if i > 0:
            # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªäºŒçº§æ ‡é¢˜ï¼Œä»é‚£é‡Œå¼€å§‹æå–å†…å®¹
            header_match = re.search(r'\n## ', content)
            if header_match:
                content = content[header_match.start():].strip()
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°äºŒçº§æ ‡é¢˜ï¼ŒæŸ¥æ‰¾ç¬¬ä¸€ä¸ªæœ‰æ„ä¹‰çš„å†…å®¹è¡Œ
                lines = content.split('\n')
                content_start = 0
                for j, line in enumerate(lines):
                    if line.strip() and not line.startswith('#') and not line.startswith('è§†é¢‘æ ‡é¢˜') and not line.startswith('è§†é¢‘æ ‡ç­¾'):
                        content_start = j
                        break
                content = '\n'.join(lines[content_start:]).strip()
        
        # ä¸ºæ¯ä¸ªåˆ†å—æ·»åŠ åˆ†èŠ‚æ ‡è¯†
        if i > 0:  # ç¬¬ä¸€ä¸ªåˆ†å—ä¸éœ€è¦é¢å¤–æ ‡è¯†
            merged_content.append(f"\n\n## ç¬¬ {section_counter} éƒ¨åˆ†ï¼ˆç»­ï¼‰\n")
            section_counter += 1
        
        # æ·»åŠ å†…å®¹ï¼Œä½†ç§»é™¤å¼€å¤´çš„æ ‡é¢˜ä¿¡æ¯
        if i == 0:
            # ç¬¬ä¸€ä¸ªå†…å®¹ï¼ŒæŸ¥æ‰¾ç¬¬ä¸€ä¸ªäºŒçº§æ ‡é¢˜å¼€å§‹çš„åœ°æ–¹
            header_match = re.search(r'\n## ', content)
            if header_match:
                main_content = content[header_match.start():].strip()
                merged_content.append(main_content)
            else:
                merged_content.append(content)
        else:
            merged_content.append(content)
    
    # æ·»åŠ åˆå¹¶è¯´æ˜
    merged_content.append(f"\n\n---\n\n## ğŸ“‹ ç”Ÿæˆè¯´æ˜\n\næœ¬ç¬”è®°ç”±äºå†…å®¹è¾ƒé•¿ï¼Œé‡‡ç”¨äº†åˆ†å—å¤„ç†å¹¶åˆå¹¶ç”Ÿæˆã€‚å…±å¤„ç†äº† {len(contents)} ä¸ªå†…å®¹åˆ†å—ã€‚")
    
    result = '\n'.join(merged_content)
    logger.info(f"âœ… å†…å®¹åˆå¹¶å®Œæˆï¼Œæœ€ç»ˆé•¿åº¦: {len(result)} å­—ç¬¦")
    
    return result


def create_chunk_summary_prompt(chunk_index: int, total_chunks: int, is_first: bool = False, is_last: bool = False) -> str:
    """
    ä¸ºåˆ†å—å¤„ç†åˆ›å»ºç‰¹æ®Šçš„promptè¯´æ˜
    
    Args:
        chunk_index: å½“å‰åˆ†å—ç´¢å¼•ï¼ˆä»1å¼€å§‹ï¼‰
        total_chunks: æ€»åˆ†å—æ•°
        is_first: æ˜¯å¦ä¸ºç¬¬ä¸€ä¸ªåˆ†å—
        is_last: æ˜¯å¦ä¸ºæœ€åä¸€ä¸ªåˆ†å—
    
    Returns:
        åˆ†å—å¤„ç†çš„promptè¯´æ˜
    """
    prompt_parts = []
    
    if is_first:
        prompt_parts.append(f"""
ğŸ“‹ **å†…å®¹åˆ†å—å¤„ç†è¯´æ˜**ï¼š
ç”±äºè§†é¢‘å†…å®¹è¾ƒé•¿ï¼Œæœ¬æ¬¡å¤„ç†é‡‡ç”¨åˆ†å—æ¨¡å¼ã€‚è¿™æ˜¯ç¬¬ {chunk_index}/{total_chunks} éƒ¨åˆ†ã€‚

å¯¹äºç¬¬ä¸€éƒ¨åˆ†ï¼Œè¯·ï¼š
1. ç”Ÿæˆå®Œæ•´çš„ç¬”è®°å¼€å¤´ï¼ˆåŒ…æ‹¬æ ‡é¢˜ã€æ¦‚è¿°ç­‰ï¼‰
2. å¤„ç†è¿™éƒ¨åˆ†çš„è¯¦ç»†å†…å®¹
3. ä¿æŒæ­£å¸¸çš„markdownç»“æ„
""")
    elif is_last:
        prompt_parts.append(f"""
ğŸ“‹ **å†…å®¹åˆ†å—å¤„ç†è¯´æ˜**ï¼š
è¿™æ˜¯ç¬¬ {chunk_index}/{total_chunks} éƒ¨åˆ†ï¼ˆæœ€åä¸€éƒ¨åˆ†ï¼‰ã€‚

å¯¹äºæœ€åéƒ¨åˆ†ï¼Œè¯·ï¼š
1. å¤„ç†è¿™éƒ¨åˆ†çš„è¯¦ç»†å†…å®¹
2. åœ¨æœ«å°¾ç”Ÿæˆå®Œæ•´çš„æ€»ç»“
3. ä¸éœ€è¦é‡å¤æ ‡é¢˜ä¿¡æ¯
""")
    else:
        prompt_parts.append(f"""
ğŸ“‹ **å†…å®¹åˆ†å—å¤„ç†è¯´æ˜**ï¼š
è¿™æ˜¯ç¬¬ {chunk_index}/{total_chunks} éƒ¨åˆ†ï¼ˆä¸­é—´éƒ¨åˆ†ï¼‰ã€‚

å¯¹äºä¸­é—´éƒ¨åˆ†ï¼Œè¯·ï¼š
1. ç›´æ¥å¤„ç†å†…å®¹ï¼Œä¸éœ€è¦é‡å¤æ ‡é¢˜
2. ä¿æŒä¸å‰åéƒ¨åˆ†çš„è¿è´¯æ€§
3. é‡ç‚¹å…³æ³¨è¿™éƒ¨åˆ†çš„æ ¸å¿ƒå†…å®¹
""")
    
    return '\n'.join(prompt_parts)