from app.gpt.base import GPT
from app.gpt.prompt_builder import generate_base_prompt
from app.models.gpt_model import GPTSource
from app.gpt.prompt import BASE_PROMPT, AI_SUM, SCREENSHOT, LINK
from app.gpt.utils import fix_markdown, estimate_tokens, estimate_mixed_content_tokens, split_segments_by_tokens, split_segments_with_images_by_tokens, merge_markdown_contents, create_chunk_summary_prompt
from app.models.transcriber_model import TranscriptSegment
from app.utils.retry_utils import retry_on_rate_limit
from datetime import timedelta
from typing import List
from app.utils.logger import get_logger

logger = get_logger(__name__)


class UniversalGPT(GPT):
    def __init__(self, client, model: str, temperature: float = 0.7):
        self.client = client
        self.model = model
        self.temperature = temperature
        self.screenshot = False
        self.screenshot = False
        self.link = False

    def _format_time(self, seconds: float) -> str:
        return str(timedelta(seconds=int(seconds)))[2:]

    def _build_segment_text(self, segments: List[TranscriptSegment]) -> str:
        """æ„å»ºè½¬å½•æ–‡æœ¬ï¼Œå¦‚æœæ²¡æœ‰è½¬å½•åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²"""
        if not segments:
            return ""
        return "\n".join(
            f"{self._format_time(seg.start)} - {seg.text.strip()}"
            for seg in segments
        )

    def ensure_segments_type(self, segments) -> List[TranscriptSegment]:
        return [TranscriptSegment(**seg) if isinstance(seg, dict) else seg for seg in segments]

    def create_messages(self, segments: List[TranscriptSegment], **kwargs):

        content_text = generate_base_prompt(
            title=kwargs.get('title'),
            segment_text=self._build_segment_text(segments),
            tags=kwargs.get('tags'),
            _format=kwargs.get('_format'),
            style=kwargs.get('style'),
            extras=kwargs.get('extras'),
        )
        
        # æ·»åŠ åˆ†å—å¤„ç†çš„ç‰¹æ®Špromptï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        chunk_prompt = kwargs.get('chunk_prompt', '')
        if chunk_prompt:
            content_text = chunk_prompt + "\n\n" + content_text

        # â›³ ç»„è£… content æ•°ç»„ï¼Œæ”¯æŒ text + image_url æ··åˆ
        content = [{"type": "text", "text": content_text}]
        video_img_urls = kwargs.get('video_img_urls', [])

        for url in video_img_urls:
            content.append({
                "type": "image_url",
                "image_url": {
                    "url": url,
                    "detail": "auto"
                }
            })

        # âœ… æ­£ç¡®æ ¼å¼ï¼šæ•´ä½“åŒ…åœ¨ä¸€ä¸ª message é‡Œï¼Œrole + content array
        messages = [{
            "role": "user",
            "content": content
        }]

        return messages

    def list_models(self):
        return self.client.models.list()

    @retry_on_rate_limit(max_retries=3, delay=30.0, backoff_factor=1.5)
    def _summarize_chunk(self, segments: List[TranscriptSegment], **kwargs) -> str:
        """å¤„ç†å•ä¸ªåˆ†å—çš„æ€»ç»“"""
        messages = self.create_messages(segments, **kwargs)
        
        # ä¼°ç®—å½“å‰promptçš„tokenæ•°
        prompt_text = str(messages)
        prompt_tokens = estimate_tokens(prompt_text)
        logger.info(f"ğŸ“Š å½“å‰åˆ†å—prompt tokenä¼°ç®—: {prompt_tokens}")
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=0.7
        )
        return response.choices[0].message.content.strip()

    @retry_on_rate_limit(max_retries=3, delay=30.0, backoff_factor=1.5)
    def summarize(self, source: GPTSource) -> str:
        self.screenshot = source.screenshot
        self.link = source.link
        source.segment = self.ensure_segments_type(source.segment)

        # æ„å»ºåŸºç¡€è½¬å½•æ–‡æœ¬
        full_segment_text = self._build_segment_text(source.segment)
        
        # åˆ›å»ºå®Œæ•´çš„promptæ–‡æœ¬ï¼ˆä¸åŒ…å«å›¾ç‰‡ï¼‰
        full_content_text = generate_base_prompt(
            title=source.title,
            segment_text=full_segment_text,
            tags=source.tags,
            _format=source._format,
            style=source.style,
            extras=source.extras,
        )
        
        # ä¼°ç®—æ€»tokenæ•°ï¼ˆåŒ…å«æ–‡æœ¬å’Œå›¾ç‰‡ï¼‰
        video_img_urls = source.video_img_urls or []
        estimated_tokens = estimate_mixed_content_tokens(full_content_text, video_img_urls)
        
        logger.info(f"ğŸ“Š æ··åˆå†…å®¹tokenä¼°ç®—: {estimated_tokens}")
        logger.info(f"ğŸ“Š å…¶ä¸­è½¬å½•æ–‡æœ¬: {estimate_tokens(full_segment_text)}")
        logger.info(f"ğŸ“Š å…¶ä¸­å›¾ç‰‡å†…å®¹: {len(video_img_urls)}å¼ å›¾ç‰‡")
        
        # è®¾ç½®tokené™åˆ¶ï¼ˆæ ¹æ®ä¸åŒæ¨¡å‹è°ƒæ•´ï¼‰
        max_tokens = 80000
        if 'gpt-4' in self.model.lower():
            max_tokens = 120000  # GPT-4æœ‰æ›´é«˜çš„é™åˆ¶
        elif 'gpt-3.5' in self.model.lower():
            max_tokens = 14000   # GPT-3.5é™åˆ¶æ›´ä½
        elif 'claude' in self.model.lower():
            max_tokens = 180000  # Claudeæœ‰å¾ˆé«˜çš„é™åˆ¶
        elif 'qwen' in self.model.lower():
            # Qwenæ¨¡å‹çš„å®é™…é™åˆ¶æ ¹æ®å…·ä½“ç‰ˆæœ¬è°ƒæ•´
            if 'qwen2.5-vl-72b' in self.model.lower():
                max_tokens = 80000   # å®é™…æµ‹è¯•å‘ç°96000æ˜¯æœ€å¤§é•¿åº¦ï¼Œä½†é¢„ç•™æ›´å¤šç©ºé—´
            # elif 'qwen2.5' in self.model.lower():
            #     max_tokens = 120000  # å…¶ä»–qwen2.5ç‰ˆæœ¬
            else:
                max_tokens = 80000   # ä¿å®ˆè®¾ç½®
        elif 'deepseek' in self.model.lower():
            max_tokens = 120000  # DeepSeeké™åˆ¶
        elif 'yi-' in self.model.lower() or 'yi_' in self.model.lower():
            max_tokens = 120000  # Yiç³»åˆ—æ¨¡å‹
        
        logger.info(f"ğŸ“Š æ¨¡å‹ {self.model} çš„tokené™åˆ¶: {max_tokens}")
        
        # å¦‚æœå†…å®¹åœ¨é™åˆ¶èŒƒå›´å†…ï¼Œç›´æ¥å¤„ç†
        if estimated_tokens <= max_tokens - 10000:  # å¢åŠ é¢„ç•™tokenç©ºé—´åˆ°10000
            logger.info("ğŸ“ å†…å®¹æœªè¶…å‡ºé™åˆ¶ï¼Œç›´æ¥å¤„ç†")
            messages = self.create_messages(
                source.segment,
                title=source.title,
                tags=source.tags,
                video_img_urls=source.video_img_urls,
                _format=source._format,
                style=source.style,
                extras=source.extras
            )
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.7
            )
            return response.choices[0].message.content.strip()
        
        # å†…å®¹è¿‡é•¿ï¼Œéœ€è¦åˆ†å—å¤„ç†
        logger.warning(f"âš ï¸ å†…å®¹è¿‡é•¿ ({estimated_tokens} tokens)ï¼Œå¯ç”¨åˆ†å—å¤„ç†æ¨¡å¼")
        
        # ä½¿ç”¨æ–°çš„æ··åˆå†…å®¹åˆ†å‰²å‡½æ•°
        chunk_results = split_segments_with_images_by_tokens(
            source.segment, 
            video_img_urls, 
            max_tokens
        )
        logger.info(f"ğŸ”„ å·²åˆ†å‰²ä¸º {len(chunk_results)} ä¸ªåˆ†å—")
        
        # å¤„ç†æ¯ä¸ªåˆ†å—
        summary_results = []
        for i, (chunk_segments, chunk_images) in enumerate(chunk_results):
            chunk_index = i + 1
            is_first = (i == 0)
            is_last = (i == len(chunk_results) - 1)
            
            logger.info(f"ğŸ”„ å¤„ç†åˆ†å— {chunk_index}/{len(chunk_results)}: {len(chunk_segments)}ä¸ªç‰‡æ®µ, {len(chunk_images)}å¼ å›¾ç‰‡")
            
            # ä¸ºå½“å‰åˆ†å—åˆ›å»ºç‰¹æ®Šçš„prompt
            chunk_prompt = create_chunk_summary_prompt(
                chunk_index=chunk_index,
                total_chunks=len(chunk_results),
                is_first=is_first,
                is_last=is_last
            )
            
            try:
                chunk_result = self._summarize_chunk(
                    chunk_segments,
                    title=source.title,
                    tags=source.tags,
                    video_img_urls=chunk_images,  # åªä¼ é€’åˆ†é…ç»™å½“å‰åˆ†å—çš„å›¾ç‰‡
                    _format=source._format,
                    style=source.style,
                    extras=source.extras,
                    chunk_prompt=chunk_prompt
                )
                
                summary_results.append(chunk_result)
                logger.info(f"âœ… åˆ†å— {chunk_index}/{len(chunk_results)} å¤„ç†å®Œæˆ")
                
            except Exception as e:
                logger.error(f"âŒ åˆ†å— {chunk_index} å¤„ç†å¤±è´¥: {e}")
                # å¦‚æœæŸä¸ªåˆ†å—å¤±è´¥ï¼Œæ·»åŠ é”™è¯¯ä¿¡æ¯
                summary_results.append(f"## ç¬¬ {chunk_index} éƒ¨åˆ†\n\n*æ­¤éƒ¨åˆ†å¤„ç†å¤±è´¥: {str(e)}*\n")
        
        # åˆå¹¶æ‰€æœ‰åˆ†å—ç»“æœ
        logger.info(f"ğŸ”— å¼€å§‹åˆå¹¶ {len(summary_results)} ä¸ªåˆ†å—ç»“æœ")
        final_result = merge_markdown_contents(summary_results)
        
        logger.info(f"âœ… åˆ†å—å¤„ç†å’Œåˆå¹¶å®Œæˆï¼Œæœ€ç»ˆå†…å®¹é•¿åº¦: {len(final_result)} å­—ç¬¦")
        return final_result
