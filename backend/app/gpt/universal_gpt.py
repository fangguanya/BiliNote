from app.gpt.base import GPT
from app.gpt.prompt_builder import generate_base_prompt
from app.models.gpt_model import GPTSource
from app.gpt.prompt import BASE_PROMPT, AI_SUM, SCREENSHOT, LINK
from app.gpt.utils import fix_markdown, estimate_tokens, split_segments_by_tokens, merge_markdown_contents, create_chunk_summary_prompt
from app.models.transcriber_model import TranscriptSegment
from app.utils.retry_utils import retry_on_rate_limit
from datetime import timedelta
from typing import List
from app.utils.logger import get_logger

logger = get_logger(__name__)


class UniversalGPT(GPT):
    def __init__(self, client, model: str, temperature: float = 0.7):
        self.client = client
        self.model = model
        self.temperature = temperature
        self.screenshot = False
        self.screenshot = False
        self.link = False

    def _format_time(self, seconds: float) -> str:
        return str(timedelta(seconds=int(seconds)))[2:]

    def _build_segment_text(self, segments: List[TranscriptSegment]) -> str:
        return "\n".join(
            f"{self._format_time(seg.start)} - {seg.text.strip()}"
            for seg in segments
        )

    def ensure_segments_type(self, segments) -> List[TranscriptSegment]:
        return [TranscriptSegment(**seg) if isinstance(seg, dict) else seg for seg in segments]

    def create_messages(self, segments: List[TranscriptSegment], **kwargs):

        content_text = generate_base_prompt(
            title=kwargs.get('title'),
            segment_text=self._build_segment_text(segments),
            tags=kwargs.get('tags'),
            _format=kwargs.get('_format'),
            style=kwargs.get('style'),
            extras=kwargs.get('extras'),
        )
        
        # æ·»åŠ åˆ†å—å¤„ç†çš„ç‰¹æ®Špromptï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        chunk_prompt = kwargs.get('chunk_prompt', '')
        if chunk_prompt:
            content_text = chunk_prompt + "\n\n" + content_text

        # â›³ ç»„è£… content æ•°ç»„ï¼Œæ”¯æŒ text + image_url æ··åˆ
        content = [{"type": "text", "text": content_text}]
        video_img_urls = kwargs.get('video_img_urls', [])

        for url in video_img_urls:
            content.append({
                "type": "image_url",
                "image_url": {
                    "url": url,
                    "detail": "auto"
                }
            })

        # âœ… æ­£ç¡®æ ¼å¼ï¼šæ•´ä½“åŒ…åœ¨ä¸€ä¸ª message é‡Œï¼Œrole + content array
        messages = [{
            "role": "user",
            "content": content
        }]

        return messages

    def list_models(self):
        return self.client.models.list()

    @retry_on_rate_limit(max_retries=3, delay=30.0, backoff_factor=1.5)
    def _summarize_chunk(self, segments: List[TranscriptSegment], **kwargs) -> str:
        """å¤„ç†å•ä¸ªåˆ†å—çš„æ€»ç»“"""
        messages = self.create_messages(segments, **kwargs)
        
        # ä¼°ç®—å½“å‰promptçš„tokenæ•°
        prompt_text = str(messages)
        prompt_tokens = estimate_tokens(prompt_text)
        logger.info(f"ğŸ“Š å½“å‰åˆ†å—prompt tokenä¼°ç®—: {prompt_tokens}")
        
        response = self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=0.7
        )
        return response.choices[0].message.content.strip()

    @retry_on_rate_limit(max_retries=3, delay=30.0, backoff_factor=1.5)
    def summarize(self, source: GPTSource) -> str:
        self.screenshot = source.screenshot
        self.link = source.link
        source.segment = self.ensure_segments_type(source.segment)

        # é¦–å…ˆä¼°ç®—æ€»tokenæ•°
        full_segment_text = self._build_segment_text(source.segment)
        estimated_tokens = estimate_tokens(full_segment_text)
        
        logger.info(f"ğŸ“Š è½¬å½•å†…å®¹tokenä¼°ç®—: {estimated_tokens}")
        
        # è®¾ç½®tokené™åˆ¶ï¼ˆæ ¹æ®ä¸åŒæ¨¡å‹è°ƒæ•´ï¼‰
        max_tokens = 80000  # é»˜è®¤é™åˆ¶
        if 'gpt-4' in self.model.lower():
            max_tokens = 120000  # GPT-4æœ‰æ›´é«˜çš„é™åˆ¶
        elif 'gpt-3.5' in self.model.lower():
            max_tokens = 14000   # GPT-3.5é™åˆ¶æ›´ä½
        elif 'claude' in self.model.lower():
            max_tokens = 180000  # Claudeæœ‰å¾ˆé«˜çš„é™åˆ¶
        
        logger.info(f"ğŸ“Š æ¨¡å‹ {self.model} çš„tokené™åˆ¶: {max_tokens}")
        
        # å¦‚æœå†…å®¹åœ¨é™åˆ¶èŒƒå›´å†…ï¼Œç›´æ¥å¤„ç†
        if estimated_tokens <= max_tokens - 5000:  # é¢„ç•™5000 tokenç»™promptæ¨¡æ¿
            logger.info("ğŸ“ å†…å®¹æœªè¶…å‡ºé™åˆ¶ï¼Œç›´æ¥å¤„ç†")
            messages = self.create_messages(
                source.segment,
                title=source.title,
                tags=source.tags,
                video_img_urls=source.video_img_urls,
                _format=source._format,
                style=source.style,
                extras=source.extras
            )
            
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=0.7
            )
            return response.choices[0].message.content.strip()
        
        # å†…å®¹è¿‡é•¿ï¼Œéœ€è¦åˆ†å—å¤„ç†
        logger.warning(f"âš ï¸ å†…å®¹è¿‡é•¿ ({estimated_tokens} tokens)ï¼Œå¯ç”¨åˆ†å—å¤„ç†æ¨¡å¼")
        
        # åˆ†å‰²segments
        segment_chunks = split_segments_by_tokens(source.segment, max_tokens)
        logger.info(f"ğŸ”„ å·²åˆ†å‰²ä¸º {len(segment_chunks)} ä¸ªåˆ†å—")
        
        # å¤„ç†æ¯ä¸ªåˆ†å—
        chunk_results = []
        for i, chunk_segments in enumerate(segment_chunks):
            chunk_index = i + 1
            is_first = (i == 0)
            is_last = (i == len(segment_chunks) - 1)
            
            logger.info(f"ğŸ”„ å¤„ç†åˆ†å— {chunk_index}/{len(segment_chunks)}: {len(chunk_segments)} ä¸ªç‰‡æ®µ")
            
            # ä¸ºå½“å‰åˆ†å—åˆ›å»ºç‰¹æ®Šçš„prompt
            chunk_prompt = create_chunk_summary_prompt(
                chunk_index=chunk_index,
                total_chunks=len(segment_chunks),
                is_first=is_first,
                is_last=is_last
            )
            
            try:
                chunk_result = self._summarize_chunk(
                    chunk_segments,
                    title=source.title,
                    tags=source.tags,
                    video_img_urls=source.video_img_urls if is_first else [],  # åªåœ¨ç¬¬ä¸€ä¸ªåˆ†å—åŒ…å«å›¾ç‰‡
                    _format=source._format,
                    style=source.style,
                    extras=source.extras,
                    chunk_prompt=chunk_prompt
                )
                
                chunk_results.append(chunk_result)
                logger.info(f"âœ… åˆ†å— {chunk_index}/{len(segment_chunks)} å¤„ç†å®Œæˆ")
                
            except Exception as e:
                logger.error(f"âŒ åˆ†å— {chunk_index} å¤„ç†å¤±è´¥: {e}")
                # å¦‚æœæŸä¸ªåˆ†å—å¤±è´¥ï¼Œæ·»åŠ é”™è¯¯ä¿¡æ¯
                chunk_results.append(f"## ç¬¬ {chunk_index} éƒ¨åˆ†\n\n*æ­¤éƒ¨åˆ†å¤„ç†å¤±è´¥: {str(e)}*\n")
        
        # åˆå¹¶æ‰€æœ‰åˆ†å—ç»“æœ
        logger.info(f"ğŸ”— å¼€å§‹åˆå¹¶ {len(chunk_results)} ä¸ªåˆ†å—ç»“æœ")
        final_result = merge_markdown_contents(chunk_results)
        
        logger.info(f"âœ… åˆ†å—å¤„ç†å’Œåˆå¹¶å®Œæˆï¼Œæœ€ç»ˆå†…å®¹é•¿åº¦: {len(final_result)} å­—ç¬¦")
        return final_result
