"""
PyTorch Whisper è½¬å†™å™¨
ä½¿ç”¨ openai-whisper åº“ï¼ŒåŸç”Ÿæ”¯æŒ CUDA 13
"""
import os
import torch
import whisper
import threading

from app.decorators.timeit import timeit
from app.models.transcriber_model import TranscriptSegment, TranscriptResult
from app.transcriber.base import Transcriber
from app.utils.logger import get_logger
from app.utils.path_helper import get_model_dir
from events import transcription_finished


logger = get_logger(__name__)

# ğŸ”’ å…¨å±€é”ï¼šä¿æŠ¤GPUæ¨¡å‹ä¸è¢«å¤šçº¿ç¨‹åŒæ—¶è®¿é—®
_whisper_lock = threading.Lock()


class WhisperTranscriber(Transcriber):
    """
    åŸºäº OpenAI Whisper (PyTorch) çš„è½¬å†™å™¨
    
    ç‰¹æ€§ï¼š
    - åŸç”Ÿæ”¯æŒ CUDA 13
    - åŸºäº PyTorchï¼Œç¨³å®šå¯é 
    - æ”¯æŒ FP16 åŠ é€Ÿ
    """
    
    def __init__(
        self,
        model_size: str = "base",
        device: str = "cuda",
        language: str = "zh",
        fp16: bool = True,
    ):
        """
        åˆå§‹åŒ– Whisper è½¬å†™å™¨
        
        Args:
            model_size: æ¨¡å‹å¤§å° (tiny/base/small/medium/large/large-v2/large-v3)
            device: è®¾å¤‡ (cuda/cpu)
            language: è¯­è¨€ä»£ç  (zh/en/auto)
            fp16: æ˜¯å¦ä½¿ç”¨ FP16 åŠ é€Ÿï¼ˆä»…CUDAå¯ç”¨ï¼‰
        """
        self.model_size = model_size
        self.language = None if language == "auto" else language
        self.fp16 = fp16 if device == "cuda" else False
        
        # ğŸ”§ è®¾å¤‡æ£€æµ‹ä¸éªŒè¯
        if device == "cuda":
            if not self._check_cuda():
                error_msg = (
                    "âŒ CUDAä¸å¯ç”¨ä½†è¢«è¦æ±‚ä½¿ç”¨GPUæ¨¡å¼ï¼\n"
                    "è¯·æ£€æŸ¥ï¼š\n"
                    "1. è¿è¡Œ nvidia-smi ç¡®è®¤GPUå¯ç”¨\n"
                    "2. ç¡®è®¤PyTorchæ”¯æŒCUDA: python -c \"import torch; print(torch.cuda.is_available())\"\n"
                    "3. å¦‚éœ€ä½¿ç”¨CPUæ¨¡å¼ï¼Œè¯·åœ¨é…ç½®ä¸­å°†deviceæ”¹ä¸º'cpu'"
                )
                logger.error(error_msg)
                raise RuntimeError("CUDAä¸å¯ç”¨ï¼Œæ— æ³•å¯åŠ¨GPUæ¨¡å¼")
            
            self.device = "cuda"
            logger.info("âœ… CUDAæ£€æµ‹é€šè¿‡ï¼Œå¼ºåˆ¶ä½¿ç”¨GPUæ¨¡å¼")
            logger.info(f"   GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}")
            logger.info(f"   CUDAç‰ˆæœ¬: {torch.version.cuda}")
            logger.info(f"   PyTorchç‰ˆæœ¬: {torch.__version__}")
            logger.info(f"   FP16åŠ é€Ÿ: {'å¯ç”¨' if self.fp16 else 'ç¦ç”¨'}")
        else:
            self.device = "cpu"
            self.fp16 = False
            logger.info("ğŸ’» ä½¿ç”¨CPUæ¨¡å¼ï¼ˆä¸æ¨èï¼Œé€Ÿåº¦è¾ƒæ…¢ï¼‰")
        
        # ğŸ”§ åŠ è½½æ¨¡å‹
        self._load_model()
    
    def _check_cuda(self) -> bool:
        """æ£€æŸ¥ CUDA æ˜¯å¦å¯ç”¨"""
        try:
            import torch
            if not torch.cuda.is_available():
                logger.error("âŒ PyTorchæ£€æµ‹ä¸åˆ°CUDA")
                return False
            
            # æµ‹è¯• CUDA æ˜¯å¦çœŸçš„å¯ç”¨
            try:
                _ = torch.zeros(1).cuda()
                logger.info("âœ… CUDAåŠŸèƒ½æµ‹è¯•é€šè¿‡")
                return True
            except Exception as e:
                logger.error(f"âŒ CUDAåŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
                return False
                
        except ImportError:
            logger.error("âŒ PyTorchæœªå®‰è£…")
            return False
    
    def _load_model(self):
        """åŠ è½½ Whisper æ¨¡å‹"""
        try:
            logger.info(f"ğŸš€ å¼€å§‹åŠ è½½ Whisper æ¨¡å‹...")
            logger.info(f"   æ¨¡å‹å¤§å°: {self.model_size}")
            logger.info(f"   è®¾å¤‡: {self.device}")
            logger.info(f"   è¯­è¨€: {self.language or 'è‡ªåŠ¨æ£€æµ‹'}")
            
            # ğŸ”§ è®¾ç½®æ¨¡å‹ä¸‹è½½ç›®å½•
            model_dir = get_model_dir("whisper")
            os.makedirs(model_dir, exist_ok=True)
            
            # ğŸ”§ åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨æ¨¡å‹åç§°ï¼Œä¸æ˜¯è·¯å¾„ï¼‰
            # openai-whisper ä¼šè‡ªåŠ¨ä¸‹è½½åˆ° download_root ç›®å½•
            logger.info(f"ğŸ”§ æ­£åœ¨åŠ è½½æ¨¡å‹åˆ° {self.device}...")
            logger.info(f"   æ¨¡å‹å°†ä¸‹è½½åˆ°: {model_dir}")
            
            self.model = whisper.load_model(
                name=self.model_size,  # ä½¿ç”¨æ¨¡å‹åç§°ï¼Œä¸æ˜¯è·¯å¾„
                device=self.device,
                download_root=model_dir,
            )
            
            logger.info(f"âœ… Whisper æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            logger.info(f"   æ¨¡å‹å‚æ•°é‡: ~{self._get_model_params()}M")
            
        except Exception as e:
            import traceback
            logger.error(f"âŒ Whisper æ¨¡å‹åŠ è½½å¤±è´¥ï¼")
            logger.error(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
            logger.error(f"   é”™è¯¯ä¿¡æ¯: {str(e)}")
            logger.error(f"   å®Œæ•´å †æ ˆ:")
            for line in traceback.format_exc().splitlines():
                logger.error(f"   {line}")
            raise
    
    def _get_model_params(self) -> int:
        """è·å–æ¨¡å‹å‚æ•°é‡ï¼ˆç™¾ä¸‡ï¼‰"""
        params_map = {
            "tiny": 39,
            "base": 74,
            "small": 244,
            "medium": 769,
            "large": 1550,
            "large-v1": 1550,
            "large-v2": 1550,
            "large-v3": 1550,
        }
        return params_map.get(self.model_size, 0)
    
    @timeit
    def transcript(self, file_path: str) -> TranscriptResult:
        """
        è½¬å†™éŸ³é¢‘æ–‡ä»¶
        
        Args:
            file_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            TranscriptResult: è½¬å†™ç»“æœ
        """
        try:
            logger.info(f"ğŸ¤ å¼€å§‹è½¬å†™éŸ³é¢‘...")
            logger.info(f"   éŸ³é¢‘è·¯å¾„: {file_path}")
            logger.info(f"   æ¨¡å‹: {self.model_size}")
            logger.info(f"   è®¾å¤‡: {self.device}")
            
            # ğŸ”§ å†æ¬¡æ£€æŸ¥ CUDA çŠ¶æ€
            if self.device == "cuda":
                if not torch.cuda.is_available():
                    raise RuntimeError("è½¬å†™æ—¶CUDAä¸å¯ç”¨ï¼")
                logger.info(f"   GPUæ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
            
            # ğŸ”’ è·å–å…¨å±€é”ï¼Œé˜²æ­¢å¤šçº¿ç¨‹åŒæ—¶ä½¿ç”¨GPUæ¨¡å‹
            logger.info(f"ğŸ”’ ç­‰å¾…è·å–GPUé”...")
            with _whisper_lock:
                logger.info(f"âœ… å·²è·å–GPUé”ï¼Œå¼€å§‹æ‰§è¡Œè½¬å†™...")
                
                # ğŸ”§ æ‰§è¡Œè½¬å†™
                result = self.model.transcribe(
                    audio=file_path,
                    language=self.language,
                    fp16=self.fp16,
                    verbose=False,  # ä¸æ‰“å°è¿›åº¦
                    task="transcribe",  # è½¬å†™ä»»åŠ¡ï¼ˆä¸æ˜¯ç¿»è¯‘ï¼‰
                )
                
                logger.info(f"ğŸ”“ è½¬å†™å®Œæˆï¼Œé‡Šæ”¾GPUé”")
            
            logger.info(f"âœ… è½¬å†™å®Œæˆï¼Œå¼€å§‹å¤„ç†ç»“æœ...")
            
            # ğŸ”§ è§£æç»“æœ
            segments = []
            full_text = ""
            
            for seg in result["segments"]:
                text = seg["text"].strip()
                full_text += text + " "
                segments.append(TranscriptSegment(
                    start=seg["start"],
                    end=seg["end"],
                    text=text
                ))
            
            detected_language = result.get("language", "unknown")
            logger.info(f"âœ… æ£€æµ‹åˆ°è¯­è¨€: {detected_language}")
            logger.info(f"âœ… è½¬å†™ç‰‡æ®µæ•°: {len(segments)}")
            logger.info(f"âœ… æ–‡æœ¬é•¿åº¦: {len(full_text)} å­—ç¬¦")
            
            transcript_result = TranscriptResult(
                language=detected_language,
                full_text=full_text.strip(),
                segments=segments,
                raw=result  # ä¿å­˜åŸå§‹ç»“æœ
            )
            
            return transcript_result
            
        except Exception as e:
            import traceback
            logger.error(f"âŒ è½¬å†™å¤±è´¥ï¼")
            logger.error(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
            logger.error(f"   é”™è¯¯ä¿¡æ¯: {str(e)}")
            logger.error(f"   éŸ³é¢‘è·¯å¾„: {file_path}")
            logger.error(f"   æ¨¡å‹: {self.model_size}")
            logger.error(f"   è®¾å¤‡: {self.device}")
            logger.error(f"   å®Œæ•´å †æ ˆ:")
            for line in traceback.format_exc().splitlines():
                logger.error(f"   {line}")
            raise  # ç›´æ¥æŠ›å‡ºå¼‚å¸¸ï¼Œä¸åšé™çº§å¤„ç†
    
    def on_finish(self, video_path: str, result: TranscriptResult) -> None:
        """è½¬å†™å®Œæˆå›è°ƒ"""
        logger.info("âœ… è½¬å†™å®Œæˆï¼Œå‘é€äº‹ä»¶é€šçŸ¥")
        transcription_finished.send({
            "file_path": video_path,
        })
    
    @staticmethod
    def is_cuda() -> bool:
        """æ£€æŸ¥ CUDA æ˜¯å¦å¯ç”¨ï¼ˆé™æ€æ–¹æ³•ï¼Œç”¨äºå…¼å®¹ï¼‰"""
        try:
            import torch
            return torch.cuda.is_available()
        except ImportError:
            return False
    
    @staticmethod
    def is_torch_installed() -> bool:
        """æ£€æŸ¥ PyTorch æ˜¯å¦å·²å®‰è£…ï¼ˆé™æ€æ–¹æ³•ï¼Œç”¨äºå…¼å®¹ï¼‰"""
        try:
            import torch
            return True
        except ImportError:
            return False
