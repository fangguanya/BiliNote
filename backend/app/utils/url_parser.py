import re
import os
import time
import json
import asyncio
from enum import Enum
from typing import List, Tuple, Optional, Dict, Union, Any
from functools import partial
import urllib.parse
from urllib.parse import urlparse, parse_qs
import requests
import yt_dlp
from app.utils.logger import get_logger
from app.utils.title_cleaner import smart_title_clean
from app.services.cookie_manager import CookieConfigManager

logger = get_logger(__name__)
cookie_manager = CookieConfigManager()


def extract_video_id(url: str, platform: str) -> Optional[str]:
    """
    ä»URLä¸­æå–è§†é¢‘ID
    """
    logger.info(f"æå–è§†é¢‘ID: {url} (å¹³å°: {platform})")
    
    if platform == "bilibili":
        # BVå¼€å¤´çš„BVå·: https://www.bilibili.com/video/BV1Ga411X7PT
        bv_match = re.search(r"(?:\/|^)(BV[a-zA-Z0-9]+)", url)
        if bv_match:
            return bv_match.group(1)
        
        # AVå¼€å¤´çš„AVå·: https://www.bilibili.com/video/av13587499
        av_match = re.search(r"\/av(\d+)", url)
        if av_match:
            return "av" + av_match.group(1)
        
    elif platform == "douyin":
        # æ ‡å‡†é“¾æ¥: https://www.douyin.com/video/7152743691239720223
        video_match = re.search(r"\/video\/(\d+)", url)
        if video_match:
            return video_match.group(1)
        
        # çŸ­é“¾æ¥å½¢å¼ï¼Œéœ€è¦å¤„ç†è·³è½¬
        if "v.douyin.com" in url:
            try:
                response = requests.head(url, allow_redirects=True, timeout=5)
                redirect_url = response.url
                logger.info(f"æŠ–éŸ³çŸ­é“¾æ¥é‡å®šå‘åˆ°: {redirect_url}")
                return extract_video_id(redirect_url, platform)
            except Exception as e:
                logger.error(f"å¤„ç†æŠ–éŸ³çŸ­é“¾æ¥å¤±è´¥: {e}")
                
    elif platform == "local":
        # æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼Œç›´æ¥è¿”å›æ–‡ä»¶å
        file_name = os.path.basename(url)
        name_without_ext = os.path.splitext(file_name)[0]
        return name_without_ext
        
    elif platform == "baidu_pan":
        # ç™¾åº¦ç½‘ç›˜é“¾æ¥å¯èƒ½ç›´æ¥ä½¿ç”¨æ–‡ä»¶å
        # è§£æURLï¼Œå°è¯•è·å–æ–‡ä»¶è·¯å¾„
        if "path=" in url:
            parsed_url = urlparse(url)
            query_params = parse_qs(parsed_url.query)
            if "path" in query_params:
                path = query_params["path"][0]
                # ä½¿ç”¨è·¯å¾„çš„æœ€åéƒ¨åˆ†ä½œä¸ºID
                return os.path.basename(path)
        
        # å¦‚æœæ— æ³•æå–è·¯å¾„ï¼Œä½¿ç”¨å½“å‰æ—¶é—´æˆ³
        return f"baidupan_{int(time.time())}"
        
    # å…¶ä»–å¹³å°æˆ–æœªèƒ½è¯†åˆ«æ—¶ï¼Œè¿”å›None
    logger.warning(f"æ— æ³•ä»URLæå–è§†é¢‘ID: {url} (å¹³å°: {platform})")
    return None


def is_video_part_of_collection(url: str) -> bool:
    """
    åˆ¤æ–­è§†é¢‘æ˜¯å¦æ˜¯åˆé›†çš„ä¸€éƒ¨åˆ†
    ä¸»è¦æ£€æŸ¥URLä¸­çš„å‚æ•° (p= æˆ– vd_source=)
    """
    # Bç«™æ ‡è®°:
    # - p=N (è¡¨ç¤ºç¬¬Nä¸ªè§†é¢‘)
    if "bilibili.com" in url and "p=" in url:
        return True
    
    # Bç«™åˆé›†æ¥æºæ ‡è®°:
    # - from_spmid=
    # - vd_source=
    if "bilibili.com" in url and ("vd_source=" in url or "from_spmid=" in url):
        # æå–vd_sourceå‚æ•°
        match = re.search(r"vd_source=([^&]+)", url)
        if match:
            vd_source = urllib.parse.unquote(match.group(1))
            # vd_sourceé€šå¸¸æ ¼å¼: xxxx_collection_xxxx æˆ– xxxx_series_xxxx
            return "collection" in vd_source or "series" in vd_source
            
    # æŠ–éŸ³åˆé›†æ ‡è®°:
    # - previous_page=particular_collection
    if "douyin.com" in url and "previous_page=particular_collection" in url:
        return True
    
    # é»˜è®¤ä¸æ˜¯åˆé›†
    return False


def is_collection_url(url: str, platform: str) -> bool:
    """
    åˆ¤æ–­URLæ˜¯å¦æ˜¯ä¸€ä¸ªåˆé›†é“¾æ¥
    è¿”å›Trueå¦‚æœæ˜¯åˆé›†é“¾æ¥ï¼Œå¦åˆ™è¿”å›False
    """
    logger.info(f"æ£€æŸ¥æ˜¯å¦ä¸ºåˆé›†URL: {url} (å¹³å°: {platform})")
    
    if platform == "bilibili":
        # Bç«™åˆé›†é“¾æ¥æ¨¡å¼
        # 1. æ”¶è—å¤¹: https://space.bilibili.com/xxx/favlist?fid=xxx
        if "space.bilibili.com" in url and ("favlist" in url or "fav" in url):
            return True
            
        # 2. ä¸“æ åˆé›†: https://space.bilibili.com/xxx/channel/seriesdetail?sid=xxx
        # 3. è§†é¢‘åˆé›†: https://space.bilibili.com/xxx/channel/collectiondetail?sid=xxx
        if "space.bilibili.com" in url and ("seriesdetail" in url or "collectiondetail" in url):
            return True
            
        # 4. ç¨åå†çœ‹: https://www.bilibili.com/watchlater/#/list
        if "bilibili.com/watchlater" in url:
            return True
            
        # 5. ç•ªå‰§ç³»åˆ—: https://www.bilibili.com/bangumi/play/ss123
        if "bilibili.com/bangumi/play/ss" in url:
            return True
            
        # 6. ç•ªå‰§åª’ä½“ä¿¡æ¯: https://www.bilibili.com/bangumi/media/md123
        if "bilibili.com/bangumi/media/md" in url:
            return True
    
    elif platform == "douyin":
        # æŠ–éŸ³ç”¨æˆ·çš„ä½œå“åˆé›†
        # https://www.douyin.com/user/MS4wLjABAAAATxxKmkv35HFMq3dXVBgZ1VR4ND3fq_hsPBqZBvz1LZo
        # æŠ–éŸ³åˆé›†é¡µ
        # https://www.douyin.com/collection/7123456789
        if "douyin.com/user" in url or "douyin.com/collection" in url:
            return True
    
    elif platform == "baidu_pan":
        # ç™¾åº¦ç½‘ç›˜åˆ†äº«é“¾æ¥
        # https://pan.baidu.com/s/xxx
        # https://pan.baidu.com/share/init?surl=xxx
        # https://pan.baidu.com/disk/main#/directory/path=%2F[ç›®å½•åç§°]
        if "pan.baidu.com" in url and ("/s/" in url or "surl=" in url or "main#/directory" in url):
            return True
    
    # é»˜è®¤ä¸æ˜¯åˆé›†
    return False


def extract_collection_videos(url: str, platform: str, max_videos: int = 50) -> List[Tuple[str, str]]:
    """
    æå–åˆé›†ä¸­çš„æ‰€æœ‰è§†é¢‘URLå’Œæ ‡é¢˜
    è¿”å›ä¸€ä¸ª(url, title)çš„åˆ—è¡¨
    ä¸ºå…¼å®¹æ€§æä¾›çš„åŒæ­¥æ¥å£
    """
    logger.info(f"ğŸš€ [åŒæ­¥æ¥å£] å¼€å§‹æå–åˆé›†è§†é¢‘: {url} (å¹³å°: {platform})")
    
    if platform == "baidu_pan":
        # ç™¾åº¦ç½‘ç›˜éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œæš‚ä¸æ”¯æŒå¼‚æ­¥
        return extract_baidu_pan_collection_videos(url, max_videos)
    
    # å…¶ä»–å¹³å°ç»Ÿä¸€ç”¨å¼‚æ­¥æ–¹æ³•å¤„ç†
    try:
        # è·å–æˆ–åˆ›å»ºäº‹ä»¶å¾ªç¯
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                 # å¦‚æœåœ¨Jupyteræˆ–ç±»ä¼¼ç¯å¢ƒä¸­ï¼Œè¿™ä¸ªæ–¹æ³•å¯èƒ½æœ‰æ•ˆ
                task = loop.create_task(extract_collection_videos_async(url, platform, max_videos))
                # æ³¨æ„ï¼šè¿™ç§æ–¹å¼åœ¨æŸäº›åœºæ™¯ä¸‹å¯èƒ½ä¸ä¼šæŒ‰é¢„æœŸå·¥ä½œï¼Œå› ä¸ºå®ƒæ²¡æœ‰é˜»å¡ç­‰å¾…ç»“æœ
                # ä½†åœ¨FastAPIçš„åŒæ­¥è·¯ç”±ä¸­ï¼Œè¿™æ˜¯ä¸ªå¤§é—®é¢˜ã€‚æ­£ç¡®çš„åšæ³•æ˜¯è·¯ç”±æœ¬èº«å°±è¯¥æ˜¯asyncçš„ã€‚
                logger.warning("  - åœ¨å·²è¿è¡Œçš„å¾ªç¯ä¸­åˆ›å»ºä»»åŠ¡ï¼Œä½†æ— æ³•åŒæ­¥ç­‰å¾…ç»“æœã€‚è¿”å›ç©ºåˆ—è¡¨ã€‚")
                return []
            else:
                return loop.run_until_complete(extract_collection_videos_async(url, platform, max_videos))
        except Exception as e:
            logger.error(f"âŒ [åŒæ­¥æ¥å£] åœ¨ç°æœ‰å¾ªç¯ä¸­è¿è¡Œå¤±è´¥: {e}", exc_info=True)
            return []
    except Exception as e:
        logger.error(f"âŒ [åŒæ­¥æ¥å£] æå–å¤±è´¥: {e}", exc_info=True)
        return []


async def extract_collection_videos_async(url: str, platform: str, max_videos: int = 50) -> List[Tuple[str, str]]:
    """
    å¼‚æ­¥æå–åˆé›†ä¸­çš„æ‰€æœ‰è§†é¢‘URLå’Œæ ‡é¢˜
    """
    logger.info(f"ğŸš€ [å¼‚æ­¥æ¥å£] å¼€å§‹æå–åˆé›†è§†é¢‘: {url} (å¹³å°: {platform})")
    
    try:
        if platform == "bilibili":
            return await _extract_bilibili_collection_videos_async(url, max_videos)
        elif platform == "douyin":
            # æŠ–éŸ³åŠŸèƒ½æš‚æœªå®Œå…¨å¼‚æ­¥åŒ–
            logger.warning("âš ï¸ æŠ–éŸ³åˆé›†æå–æš‚æœªå®Œå…¨å¼‚æ­¥åŒ–")
            return []
        elif platform == "baidu_pan":
             # ç™¾åº¦ç½‘ç›˜åŠŸèƒ½æš‚æœªå®Œå…¨å¼‚æ­¥åŒ–
            logger.warning("âš ï¸ ç™¾åº¦ç½‘ç›˜åˆé›†æå–æš‚æœªå®Œå…¨å¼‚æ­¥åŒ–")
            return []
        else:
            logger.warning(f"âš ï¸ ä¸æ”¯æŒçš„åˆé›†å¹³å°: {platform}")
            return []
                
    except Exception as e:
        logger.error(f"âŒ æå–åˆé›†è§†é¢‘å¤±è´¥: {e}", exc_info=True)
        return []


async def _extract_bilibili_collection_videos_async(url: str, max_videos: int = 50) -> List[Tuple[str, str]]:
    """
    Bç«™åˆé›†æå–æ ¸å¿ƒé€»è¾‘ï¼ˆå¼‚æ­¥ï¼‰
    """
    logger.info(f" Bç«™åˆé›†æå–æ ¸å¿ƒé€»è¾‘ (å¼‚æ­¥): {url}")
    
    try:
        logger.info("  - å°è¯•ä½¿ç”¨ yt-dlp æå– (å¼‚æ­¥)...")
        videos = await _extract_bilibili_video_collection_via_ytdlp_async(url, max_videos)
        if videos:
            logger.info(f"âœ… yt-dlp æå–æˆåŠŸï¼Œå…± {len(videos)} ä¸ªè§†é¢‘")
            return videos
        else:
            logger.info("  - yt-dlp æå–å¤±è´¥æˆ–æ— ç»“æœï¼Œå°è¯•å¤‡ç”¨APIæ–¹æ³•")
    except Exception as e:
        logger.warning(f"âš ï¸ yt-dlp æå–å¤±è´¥: {e}ï¼Œå°è¯•å¤‡ç”¨APIæ–¹æ³•")

    try:
        logger.info("  - å°è¯•ä½¿ç”¨ Bç«™API æå– (å¼‚æ­¥)...")
        videos = await _extract_bilibili_collection_by_api_async(url, max_videos)
        if videos:
            logger.info(f"âœ… Bç«™API æå–æˆåŠŸï¼Œå…± {len(videos)} ä¸ªè§†é¢‘")
            return videos
        else:
            logger.info("  - Bç«™API æå–å¤±è´¥æˆ–æ— ç»“æœ")
            return []
    except Exception as e:
        logger.error(f"âŒ Bç«™API æå–ä¹Ÿå¤±è´¥: {e}", exc_info=True)
        return []

async def _extract_bilibili_video_collection_via_ytdlp_async(url: str, max_videos: int = 50) -> List[Tuple[str, str]]:
    """
    ä½¿ç”¨ yt-dlp å¼‚æ­¥æå–Bç«™åˆé›†è§†é¢‘
    """
    logger.info(f"  - [yt-dlp-async] æ­£åœ¨å¤„ç†: {url}")
    loop = asyncio.get_running_loop()
    ydl_opts = {
        'quiet': True, 'extract_flat': 'in_playlist', 'playlistend': max_videos,
        'logger': logger, 'skip_download': True, 'ignoreerrors': True,
        'bilibili_api': 'web', 'format': 'bestvideo', 'socket_timeout': 15
    }
    bilibili_cookie = cookie_manager.get("bilibili")
    if bilibili_cookie:
        logger.info("  - [yt-dlp-async] ä½¿ç”¨å·²ä¿å­˜çš„Bç«™ç™»å½•cookie")
        ydl_opts['cookiefile'] = cookie_manager.get_cookie_file_path("bilibili")
    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            extract_func = partial(ydl.extract_info, url, download=False)
            info_dict = await loop.run_in_executor(None, extract_func)
        videos = []
        if info_dict and 'entries' in info_dict:
            for entry in info_dict.get('entries', []):
                if entry and entry.get('url') and entry.get('title'):
                    videos.append((entry['url'], smart_title_clean(entry['title'])))
        return videos
    except Exception as e:
        logger.warning(f"  - [yt-dlp-async] å¼‚æ­¥æå–çº¿ç¨‹å‡ºé”™: {e}", exc_info=True)
        return []

async def _async_get_request(url: str, headers: dict, timeout: int = 10):
    """å¼‚æ­¥æ‰§è¡Œ requests.get"""
    loop = asyncio.get_running_loop()
    return await loop.run_in_executor(
        None,
        partial(requests.get, url, headers=headers, timeout=timeout)
    )

async def _extract_bilibili_collection_by_api_async(url: str, max_videos: int = 50) -> List[Tuple[str, str]]:
    logger.info(f"ğŸŒ [API-async] ä½¿ç”¨Bç«™APIæå–: {url}")
    if "favlist" in url:
        fid_match = re.search(r"fid=(\d+)", url)
        if fid_match:
            api_url = f"https://api.bilibili.com/x/v3/fav/resource/list?media_id={fid_match.group(1)}&pn=1&ps={max_videos}"
            return await _fetch_bilibili_favlist_videos_async(api_url)
    elif "collectiondetail" in url or "seriesdetail" in url:
        sid_match = re.search(r"sid=(\d+)", url)
        if sid_match:
            api_url = f"https://api.bilibili.com/x/polymer/space/seasons_archives_list?mid=0&season_id={sid_match.group(1)}&sort_reverse=false&page_num=1&page_size={max_videos}"
            return await _fetch_bilibili_collection_videos_async(api_url)
    elif "watchlater" in url:
        api_url = f"https://api.bilibili.com/x/v2/history/toview?ps={max_videos}&pn=1"
        return await _fetch_bilibili_watchlater_videos_async(api_url)
    elif "bangumi/play/ss" in url:
        ss_match = re.search(r"ss(\d+)", url)
        if ss_match:
            api_url = f"https://api.bilibili.com/pgc/web/season/section?season_id={ss_match.group(1)}"
            return await _fetch_bilibili_bangumi_videos_async(api_url)
    elif "bangumi/media/md" in url:
        md_match = re.search(r"md(\d+)", url)
        if md_match:
            media_api_url = f"https://api.bilibili.com/pgc/review/user?media_id={md_match.group(1)}"
            return await _fetch_bilibili_bangumi_by_media_id_async(media_api_url, max_videos)
    logger.warning("âš ï¸ [API-async] æœªè¯†åˆ«çš„Bç«™åˆé›†ç±»å‹")
    return []

async def _fetch_bilibili_favlist_videos_async(api_url: str) -> List[Tuple[str, str]]:
    logger.info(f"ğŸ“¡ [API-async] è¯·æ±‚æ”¶è—å¤¹API: {api_url}")
    headers = {'User-Agent': 'Mozilla/5.0...', 'Referer': 'https://www.bilibili.com/'}
    if cookie := cookie_manager.get("bilibili"): headers['Cookie'] = cookie
    try:
        response = await _async_get_request(api_url, headers)
        data = response.json()
        if data.get('code') == 0 and data.get('data', {}).get('medias'):
            return [(f"https://www.bilibili.com/video/{m['bvid']}", m['title']) for m in data['data']['medias'] if m.get('bvid') and m.get('title')]
    except Exception as e:
        logger.error(f"âŒ [API-async] è·å–æ”¶è—å¤¹å¤±è´¥: {e}")
    return []

async def _fetch_bilibili_collection_videos_async(api_url: str) -> List[Tuple[str, str]]:
    logger.info(f"ğŸ“¡ [API-async] è¯·æ±‚åˆé›†API: {api_url}")
    headers = {'User-Agent': 'Mozilla/5.0...', 'Referer': 'https://www.bilibili.com/'}
    if cookie := cookie_manager.get("bilibili"): headers['Cookie'] = cookie
    try:
        response = await _async_get_request(api_url, headers)
        data = response.json()
        if data.get('code') == 0 and data.get('data', {}).get('archives'):
            return [(f"https://www.bilibili.com/video/{a['bvid']}", a['title']) for a in data['data']['archives'] if a.get('bvid') and a.get('title')]
    except Exception as e:
        logger.error(f"âŒ [API-async] è·å–åˆé›†è§†é¢‘å¤±è´¥: {e}")
    return []

async def _fetch_bilibili_watchlater_videos_async(api_url: str) -> List[Tuple[str, str]]:
    logger.info(f"ğŸ“¡ [API-async] è¯·æ±‚ç¨åå†çœ‹API: {api_url}")
    headers = {'User-Agent': 'Mozilla/5.0...', 'Referer': 'https://www.bilibili.com/'}
    if not (cookie := cookie_manager.get("bilibili")):
        logger.warning("âš ï¸ [API-async] ç¨åå†çœ‹éœ€è¦ç™»å½•cookie")
        return []
    headers['Cookie'] = cookie
    try:
        response = await _async_get_request(api_url, headers)
        data = response.json()
        if data.get('code') == 0 and data.get('data', {}).get('list'):
            return [(f"https://www.bilibili.com/video/{v['bvid']}", v['title']) for v in data['data']['list'] if v.get('bvid') and v.get('title')]
    except Exception as e:
        logger.error(f"âŒ [API-async] è·å–ç¨åå†çœ‹å¤±è´¥: {e}")
    return []

async def _fetch_bilibili_bangumi_videos_async(api_url: str) -> List[Tuple[str, str]]:
    logger.info(f"ğŸ“¡ [API-async] è¯·æ±‚ç•ªå‰§API: {api_url}")
    headers = {'User-Agent': 'Mozilla/5.0...', 'Referer': 'https://www.bilibili.com/'}
    if cookie := cookie_manager.get("bilibili"): headers['Cookie'] = cookie
    try:
        response = await _async_get_request(api_url, headers)
        data = response.json()
        videos = []
        if data.get('code') == 0 and 'result' in data:
            sections = data['result'].get('section', [])
            if not sections and 'main_section' in data['result']: sections = [data['result']['main_section']]
            for section in sections:
                for ep in section.get('episodes', []):
                    if ep.get('bvid') and ep.get('long_title'):
                        title = f"{ep.get('title', '')} {ep.get('long_title', '')}".strip()
                        videos.append((f"https://www.bilibili.com/video/{ep['bvid']}", title))
        return videos
    except Exception as e:
        logger.error(f"âŒ [API-async] è·å–ç•ªå‰§å¤±è´¥: {e}")
    return []

async def _fetch_bilibili_bangumi_by_media_id_async(api_url: str, max_videos: int) -> List[Tuple[str, str]]:
    logger.info(f"ğŸ“¡ [API-async] è¯·æ±‚ç•ªå‰§åª’ä½“API: {api_url}")
    headers = {'User-Agent': 'Mozilla/5.0...', 'Referer': 'https://www.bilibili.com/'}
    if cookie := cookie_manager.get("bilibili"): headers['Cookie'] = cookie
    try:
        response = await _async_get_request(api_url, headers)
        data = response.json()
        if data.get('code') == 0 and data.get('result', {}).get('media', {}).get('season_id'):
            season_id = data['result']['media']['season_id']
            # æœ‰äº†season_idï¼Œå°±èƒ½æ‹¿åˆ°ç•ªå‰§è¯¦æƒ…
            season_api = f"https://api.bilibili.com/pgc/web/season/section?season_id={season_id}"
            return await _fetch_bilibili_bangumi_videos_async(season_api)
    except Exception as e:
        logger.error(f"âŒ [API-async] è·å–ç•ªå‰§åª’ä½“ä¿¡æ¯å¤±è´¥: {e}")
    return []


def identify_platform(url: str) -> Optional[str]:
    """
    è¯†åˆ«URLå±äºå“ªä¸ªå¹³å°
    è¿”å›å¹³å°æ ‡è¯†ç¬¦ï¼ˆbilibili, douyinç­‰ï¼‰æˆ–Noneè¡¨ç¤ºæ— æ³•è¯†åˆ«
    """
    # å¤„ç†æœ¬åœ°æ–‡ä»¶
    if not url.startswith(("http://", "https://")):
        if os.path.exists(url) or url.startswith(("file://", "/", "C:\\", "D:\\")):
            return "local"
    
    # å¤„ç†å„å¤§è§†é¢‘å¹³å°
    if "bilibili.com" in url:
        return "bilibili"
    elif "douyin.com" in url or "iesdouyin.com" in url:
        return "douyin"
    elif "pan.baidu.com" in url:
        return "baidu_pan"
    
    # ä¸æ”¯æŒçš„å¹³å°
    return None


def extract_baidu_pan_collection_videos(url: str, max_videos: int = 50) -> List[Tuple[str, str]]:
    """
    æå–ç™¾åº¦ç½‘ç›˜åˆ†äº«é“¾æ¥ä¸­çš„è§†é¢‘æ–‡ä»¶
    è¿”å›æ–‡ä»¶åˆ—è¡¨(url, title)
    
    ç™¾åº¦ç½‘ç›˜å¤„ç†æ¯”è¾ƒç‰¹æ®Šï¼Œéœ€è¦ç»è¿‡èº«ä»½éªŒè¯ï¼Œæ­¤å¤„ç®€å•å®ç°
    å®é™…åº”ç”¨ä¸­éœ€è¦å¤„ç†èº«ä»½éªŒè¯ã€æå–ç ç­‰
    """
    logger.info(f"ğŸŒ [ç™¾åº¦ç½‘ç›˜] å¤„ç†ç½‘ç›˜é“¾æ¥: {url}")
    
    # è¿™é‡Œå®é™…ä¸Šåº”è¯¥è¿”å›çœŸå®çš„æ–‡ä»¶åˆ—è¡¨
    # ä½†ç”±äºç™¾åº¦ç½‘ç›˜çš„APIè®¿é—®éœ€è¦å¤æ‚çš„èº«ä»½éªŒè¯å’Œæˆæƒ
    # è¿™é‡Œåªæ˜¯è¿”å›ä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œå®é™…ä¸šåŠ¡åº”è¯¥ç”±ç™¾åº¦ç½‘ç›˜APIä¸“é—¨æ¨¡å—å¤„ç†
    
    from app.services.baidu_pan import BaiduPanService
    
    try:
        # é€šè¿‡æœåŠ¡è·å–æ–‡ä»¶åˆ—è¡¨
        baidu_service = BaiduPanService()
        if "share/init" in url or "/s/" in url:
            logger.info("æ£€æµ‹åˆ°ç™¾åº¦ç½‘ç›˜åˆ†äº«é“¾æ¥")
            
            # æå–åˆ†äº«ç 
            share_code = None
            if "/s/" in url:
                share_code = url.split("/s/")[1].split("?")[0]
            elif "surl=" in url:
                share_code = re.search(r"surl=([^&]+)", url).group(1)
                
            # æå–æå–ç 
            extract_code = None
            if "?pwd=" in url:
                extract_code = url.split("?pwd=")[1].split("&")[0]
                
            logger.info(f"åˆ†äº«ç : {share_code}, æå–ç : {extract_code}")
            
            # è·å–åˆ†äº«é“¾æ¥ä¸­çš„æ–‡ä»¶åˆ—è¡¨
            files = baidu_service.list_shared_files(share_code, extract_code)
            if files:
                # è¿‡æ»¤è§†é¢‘æ–‡ä»¶
                video_files = [(f"{share_code}|{f['fs_id']}|{f['path']}", f['server_filename']) 
                              for f in files if f.get('category') == 1]  # 1è¡¨ç¤ºè§†é¢‘
                return video_files[:max_videos]
        else:
            # ä¸ªäººç½‘ç›˜ç›®å½•
            if "path=" in url:
                path = urllib.parse.unquote(re.search(r"path=([^&]+)", url).group(1))
                logger.info(f"ç™¾åº¦ç½‘ç›˜ç›®å½•: {path}")
                
                # è·å–ä¸ªäººç½‘ç›˜ä¸­çš„æ–‡ä»¶åˆ—è¡¨
                files = baidu_service.list_files(path)
                if files:
                    # è¿‡æ»¤è§†é¢‘æ–‡ä»¶
                    video_files = [(f"personal|{f['fs_id']}|{f['path']}", f['server_filename']) 
                                  for f in files if f.get('category') == 1]  # 1è¡¨ç¤ºè§†é¢‘
                    return video_files[:max_videos]
                
    except Exception as e:
        logger.error(f"âŒ è·å–ç™¾åº¦ç½‘ç›˜æ–‡ä»¶å¤±è´¥: {e}", exc_info=True)
        
    # è‹¥æ— æ³•è·å–çœŸå®æ–‡ä»¶ï¼Œè¿”å›ç©ºåˆ—è¡¨
    return []
