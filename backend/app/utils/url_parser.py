import re
from typing import Optional, List, Tuple
import yt_dlp
import requests
from urllib.parse import urlparse, parse_qs
import os

# æ·»åŠ æ—¥å¿—æ”¯æŒ
from app.utils.logger import get_logger
from app.services.cookie_manager import CookieConfigManager
from app.utils.title_cleaner import smart_title_clean

logger = get_logger(__name__)

# åˆå§‹åŒ–Cookieç®¡ç†å™¨
cookie_manager = CookieConfigManager()

def extract_video_id(url: str, platform: str) -> Optional[str]:
    """
    ä»è§†é¢‘é“¾æ¥ä¸­æå–è§†é¢‘ ID

    :param url: è§†é¢‘é“¾æ¥
    :param platform: å¹³å°åï¼ˆbilibili / youtube / douyin / baidu_panï¼‰
    :return: æå–åˆ°çš„è§†é¢‘ ID æˆ– None
    """
    if platform == "bilibili":
        # åŒ¹é… BVå·ï¼ˆå¦‚ BV1vc411b7Waï¼‰
        match = re.search(r"BV([0-9A-Za-z]+)", url)
        return f"BV{match.group(1)}" if match else None

    elif platform == "youtube":
        # åŒ¹é… v=xxxxx æˆ– youtu.be/xxxxxï¼ŒID é•¿åº¦é€šå¸¸ä¸º 11
        match = re.search(r"(?:v=|youtu\.be/)([0-9A-Za-z_-]{11})", url)
        return match.group(1) if match else None

    elif platform == "douyin":
        # åŒ¹é… douyin.com/video/1234567890123456789
        match = re.search(r"/video/(\d+)", url)
        return match.group(1) if match else None

    elif platform == "baidu_pan":
        # ç™¾åº¦ç½‘ç›˜åˆ†äº«é“¾æ¥ï¼šhttps://pan.baidu.com/s/1ABC123DEF
        # æˆ–ç›®å½•é“¾æ¥ï¼šhttps://pan.baidu.com/disk/home#/path=/è§†é¢‘ç›®å½•
        share_match = re.search(r"/s/([0-9A-Za-z_-]+)", url)
        if share_match:
            return share_match.group(1)
        
        # ç›®å½•è·¯å¾„æå–
        path_match = re.search(r"#/path=([^&]+)", url)
        if path_match:
            return path_match.group(1)
        
        # æ–‡ä»¶fsidæå–ï¼ˆç”¨äºç‰¹å®šæ–‡ä»¶ï¼‰
        fsid_match = re.search(r"fsid=(\d+)", url)
        if fsid_match:
            return fsid_match.group(1)
        
        return None

    return None


def is_video_part_of_collection(url: str) -> bool:
    """
    æ£€æŸ¥å•ä¸ªBç«™è§†é¢‘æ˜¯å¦å±äºæŸä¸ªåˆé›†
    å‚è€ƒBilibiliDowné¡¹ç›®çš„åˆé›†æ£€æµ‹é€»è¾‘
    
    :param url: è§†é¢‘é“¾æ¥
    :return: æ˜¯å¦å±äºåˆé›†
    """
    logger.info(f"ğŸ” æ£€æŸ¥è§†é¢‘æ˜¯å¦å±äºåˆé›†: {url}")
    
    # æå–BVå·
    bv_match = re.search(r"BV([0-9A-Za-z]+)", url)
    if not bv_match:
        logger.info("âŒ æ— æ³•æå–BVå·")
        return False
    
    bv_id = f"BV{bv_match.group(1)}"
    logger.info(f"ğŸ“¹ æå–åˆ°BVå·: {bv_id}")
    
    # ä½¿ç”¨Bç«™APIè¿›è¡Œå‡†ç¡®æ£€æµ‹
    try:
        logger.info(f"ğŸŒ ä½¿ç”¨Bç«™APIæ£€æŸ¥è§†é¢‘åˆé›†ä¿¡æ¯...")
        
        import requests
        
        # ä½¿ç”¨Bç«™è§†é¢‘ä¿¡æ¯API
        api_url = f"https://api.bilibili.com/x/web-interface/view?bvid={bv_id}"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://www.bilibili.com/'
        }
        
        # æ·»åŠ ç™»å½•cookieæ”¯æŒ
        bilibili_cookie = cookie_manager.get("bilibili")
        if bilibili_cookie:
            logger.info("ğŸª ä½¿ç”¨å·²ä¿å­˜çš„Bç«™ç™»å½•cookie")
            headers['Cookie'] = bilibili_cookie
        
        response = requests.get(api_url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            
            if data.get('code') == 0 and 'data' in data:
                video_data = data['data']
                logger.info(f"ğŸ“Š æˆåŠŸè·å–è§†é¢‘ä¿¡æ¯: {video_data.get('title', 'æœªçŸ¥æ ‡é¢˜')}")
                
                # æ£€æŸ¥1: æ˜¯å¦å±äºUGCåˆé›†ï¼ˆugc_seasonï¼‰
                if 'ugc_season' in video_data and video_data['ugc_season']:
                    season_info = video_data['ugc_season']
                    season_title = season_info.get('title', 'æœªçŸ¥åˆé›†')
                    logger.info(f"âœ… è§†é¢‘ {bv_id} å±äºUGCåˆé›†: {season_title}")
                    return True
                
                # æ£€æŸ¥2: æ˜¯å¦æœ‰å¤šåˆ†Pä¸”æ•°é‡è¾ƒå¤šï¼ˆ3ä¸ªä»¥ä¸Šè®¤ä¸ºæ˜¯åˆé›†ï¼‰
                if 'pages' in video_data and len(video_data['pages']) > 2:
                    page_count = len(video_data['pages'])
                    logger.info(f"âœ… è§†é¢‘ {bv_id} æœ‰ {page_count} ä¸ªåˆ†Pï¼Œè®¤ä¸ºæ˜¯åˆé›†")
                    return True
                
                # æ£€æŸ¥3: æ˜¯å¦å±äºç•ªå‰§/ç”µå½±ç­‰ï¼ˆseasonå­—æ®µï¼‰
                if 'season' in video_data and video_data['season']:
                    season_info = video_data['season']
                    season_title = season_info.get('title', 'æœªçŸ¥ç•ªå‰§')
                    logger.info(f"âœ… è§†é¢‘ {bv_id} å±äºç•ªå‰§: {season_title}")
                    return True
                
                # æ£€æŸ¥4: æ˜¯å¦å±äºç³»åˆ—è§†é¢‘ï¼ˆé€šè¿‡upä¸»çš„å…¶ä»–è§†é¢‘åˆ¤æ–­ï¼‰
                if 'owner' in video_data:
                    owner_mid = video_data['owner'].get('mid')
                    video_title = video_data.get('title', '')
                    
                    # å¦‚æœæ ‡é¢˜åŒ…å«æ˜æ˜¾çš„ç³»åˆ—æ ‡è¯†ï¼Œä¹Ÿè®¤ä¸ºæ˜¯åˆé›†
                    series_keywords = ['åˆé›†', 'ç³»åˆ—', 'ç¬¬ä¸€é›†', 'ç¬¬äºŒé›†', 'P1', 'P2', 'ä¸Šç¯‡', 'ä¸‹ç¯‡', 'ï¼ˆä¸€ï¼‰', 'ï¼ˆäºŒï¼‰', 
                                     'ã€åˆé›†ã€‘', 'ã€ç³»åˆ—ã€‘', 'å…¨é›†', 'è¿è½½', 'ç•ªå¤–', 'EP', 'ep']
                    if any(keyword in video_title for keyword in series_keywords):
                        logger.info(f"âœ… è§†é¢‘ {bv_id} æ ‡é¢˜åŒ…å«ç³»åˆ—å…³é”®è¯ï¼Œè®¤ä¸ºæ˜¯åˆé›†")
                        return True
                
                # æ£€æŸ¥5: å°è¯•æ£€æŸ¥æ˜¯å¦æœ‰ç›¸å…³çš„åˆé›†ä¿¡æ¯
                # ä½¿ç”¨è§†é¢‘è¯¦ç»†ä¿¡æ¯APIè·å–æ›´å¤šæ•°æ®
                detail_api_url = f"https://api.bilibili.com/x/web-interface/view/detail?bvid={bv_id}"
                detail_headers = headers.copy()
                if bilibili_cookie:
                    detail_headers['Cookie'] = bilibili_cookie
                detail_response = requests.get(detail_api_url, headers=detail_headers, timeout=8)
                
                if detail_response.status_code == 200:
                    detail_data = detail_response.json()
                    if detail_data.get('code') == 0 and 'data' in detail_data:
                        detail_info = detail_data['data']
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸å…³è§†é¢‘ä¿¡æ¯
                        if 'Related' in detail_info and len(detail_info['Related']) > 3:
                            logger.info(f"âœ… è§†é¢‘ {bv_id} æœ‰è¾ƒå¤šç›¸å…³è§†é¢‘ï¼Œå¯èƒ½æ˜¯ç³»åˆ—å†…å®¹")
                            return True
                            
        else:
            logger.warning(f"âš ï¸ Bç«™APIè¯·æ±‚å¤±è´¥: {response.status_code}")
            
    except Exception as e:
        logger.warning(f"âš ï¸ Bç«™APIæ£€æŸ¥å¤±è´¥: {e}")
    
    # å¦‚æœæ‰€æœ‰æ£€æŸ¥éƒ½æ— æ³•ç¡®å®šï¼Œä¿å®ˆåœ°è®¤ä¸ºä¸æ˜¯åˆé›†
    logger.info(f"âŒ è§†é¢‘ {bv_id} é€šè¿‡å¤šç§æ–¹æ³•æ£€æŸ¥åï¼Œæ— æ³•ç¡®å®šæ˜¯å¦å±äºåˆé›†ï¼Œé»˜è®¤ä¸ºå•è§†é¢‘")
    return False


def is_collection_url(url: str, platform: str) -> bool:
    """
    æ£€æµ‹æ˜¯å¦ä¸ºåˆé›†URL
    
    æ”¯æŒçš„åˆé›†ç±»å‹ï¼š
    - Bç«™ï¼šæ”¶è—å¤¹ã€ä¸ªäººåˆé›†ã€ç³»åˆ—è§†é¢‘ã€ç¨åå†çœ‹ã€ç•ªå‰§ç³»åˆ—ã€å¤šåˆ†Pè§†é¢‘ã€UGCåˆé›†ã€ç”¨æˆ·æŠ•ç¨¿ã€é¢‘é“é¦–é¡µ
    - æŠ–éŸ³ï¼šç”¨æˆ·ä¸»é¡µã€è¯é¢˜é¡µé¢
    - ç™¾åº¦ç½‘ç›˜ï¼šç›®å½•ã€åˆ†äº«æ–‡ä»¶å¤¹
    
    :param url: è§†é¢‘é“¾æ¥  
    :param platform: å¹³å°å
    :return: æ˜¯å¦ä¸ºåˆé›†
    """
    logger.info(f"ğŸ” æ£€æµ‹åˆé›†URL: {url} (å¹³å°: {platform})")
    
    if platform == "bilibili":
        # Bç«™åˆé›†æ£€æµ‹æ¨¡å¼ï¼ˆå·²æ”¹è¿›ï¼‰
        collection_patterns = [
            r"favlist\?fid=",                    # æ”¶è—å¤¹
            r"collectiondetail\?sid=",           # ä¸ªäººåˆé›†
            r"seriesdetail\?sid=",               # ç³»åˆ—è§†é¢‘  
            r"watchlater",                       # ç¨åå†çœ‹
            r"bangumi/play/ss\d+",               # ç•ªå‰§ç³»åˆ—
            r"bangumi/media/md\d+",              # ç•ªå‰§åª’ä½“
            r"space\.bilibili\.com/\d+/video",   # ç”¨æˆ·æŠ•ç¨¿é¡µ
            r"channel/index",                    # é¢‘é“é¦–é¡µ
        ]
        
        for i, pattern in enumerate(collection_patterns):
            if re.search(pattern, url):
                pattern_names = ["æ”¶è—å¤¹", "ä¸ªäººåˆé›†", "ç³»åˆ—è§†é¢‘", "ç¨åå†çœ‹", "ç•ªå‰§ç³»åˆ—", "ç•ªå‰§åª’ä½“", "ç”¨æˆ·æŠ•ç¨¿", "é¢‘é“é¦–é¡µ"]
                logger.info(f"âœ… æ£€æµ‹åˆ°Bç«™{pattern_names[i]}é“¾æ¥: {pattern}")
                return True
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºå¤šåˆ†Pè§†é¢‘æˆ–å±äºåˆé›†çš„å•è§†é¢‘
        if re.search(r"bilibili\.com/video/BV", url):
            logger.info("ğŸ” æ£€æµ‹åˆ°Bç«™è§†é¢‘ï¼Œæ£€æŸ¥æ˜¯å¦å±äºåˆé›†")
            # è°ƒç”¨è¯¦ç»†çš„åˆé›†æ£€æµ‹å‡½æ•°
            return is_video_part_of_collection(url)
        
        logger.info("âŒ ä¸æ˜¯Bç«™åˆé›†é“¾æ¥")
        return False
    
    elif platform == "douyin":
        # æŠ–éŸ³åˆé›†æ£€æµ‹æ¨¡å¼ï¼ˆç”¨æˆ·ä¸»é¡µæˆ–è¯é¢˜é¡µï¼‰
        collection_patterns = [
            r"douyin\.com/user/",  # ç”¨æˆ·ä¸»é¡µ
            r"douyin\.com/hashtag/",  # è¯é¢˜é¡µé¢
        ]
        
        for i, pattern in enumerate(collection_patterns):
            if re.search(pattern, url):
                pattern_names = ["ç”¨æˆ·ä¸»é¡µ", "è¯é¢˜é¡µé¢"]
                logger.info(f"âœ… æ£€æµ‹åˆ°æŠ–éŸ³{pattern_names[i]}é“¾æ¥: {pattern}")
                return True
        
        logger.info("âŒ ä¸æ˜¯æŠ–éŸ³åˆé›†é“¾æ¥")
        return False
    
    elif platform == "baidu_pan":
        # ç™¾åº¦ç½‘ç›˜åˆé›†æ£€æµ‹æ¨¡å¼
        collection_patterns = [
            r"#/path=/",                         # ç›®å½•è·¯å¾„
            r"/disk/home",                       # ä¸ªäººç½‘ç›˜ä¸»é¡µ
            r"dir\?path=",                       # ç›®å½•å‚æ•°
        ]
        
        for i, pattern in enumerate(collection_patterns):
            if re.search(pattern, url):
                pattern_names = ["ç›®å½•è·¯å¾„", "ç½‘ç›˜ä¸»é¡µ", "ç›®å½•å‚æ•°"]
                logger.info(f"âœ… æ£€æµ‹åˆ°ç™¾åº¦ç½‘ç›˜{pattern_names[i]}é“¾æ¥: {pattern}")
                return True
        
        # åˆ†äº«é“¾æ¥é»˜è®¤ä¹Ÿå¯èƒ½åŒ…å«å¤šä¸ªæ–‡ä»¶
        if re.search(r"/s/[0-9A-Za-z_-]+", url):
            logger.info("âœ… æ£€æµ‹åˆ°ç™¾åº¦ç½‘ç›˜åˆ†äº«é“¾æ¥ï¼Œå¯èƒ½åŒ…å«å¤šä¸ªæ–‡ä»¶")
            return True
        
        logger.info("âŒ ä¸æ˜¯ç™¾åº¦ç½‘ç›˜åˆé›†é“¾æ¥")
        return False
    
    logger.info(f"âŒ ä¸æ”¯æŒçš„å¹³å°: {platform}")
    return False


def extract_collection_videos(url: str, platform: str, max_videos: int = 50) -> List[Tuple[str, str]]:
    """
    æå–åˆé›†ä¸­çš„æ‰€æœ‰è§†é¢‘
    
    :param url: åˆé›†é“¾æ¥
    :param platform: å¹³å°å
    :param max_videos: æœ€å¤§è§†é¢‘æ•°é‡
    :return: [(video_url, title), ...] åˆ—è¡¨
    """
    logger.info(f"ğŸ¬ å¼€å§‹æå–åˆé›†è§†é¢‘: {url} (å¹³å°: {platform}, æœ€å¤§æ•°é‡: {max_videos})")
    
    if platform == "bilibili":
        return _extract_bilibili_collection_videos(url, max_videos)
    elif platform == "douyin":
        return _extract_douyin_collection_videos(url, max_videos)
    elif platform == "baidu_pan":
        return extract_baidu_pan_collection_videos(url, max_videos)
    else:
        logger.warning(f"âš ï¸ ä¸æ”¯æŒçš„å¹³å°: {platform}")
        return []


def _extract_bilibili_collection_videos(url: str, max_videos: int = 50) -> List[Tuple[str, str]]:
    """
    æå–Bç«™åˆé›†ä¸­çš„è§†é¢‘
    """
    logger.info(f"ğŸ”§ ä½¿ç”¨Bç«™æå–å™¨å¤„ç†: {url}")
    videos = []
    
    try:
        # æ£€æŸ¥æ˜¯å¦æ˜¯å•è§†é¢‘å±äºåˆé›†çš„æƒ…å†µ
        video_pattern = r"bilibili\.com/video/[A-Za-z0-9]+"
        if re.search(video_pattern, url) and is_video_part_of_collection(url):
            logger.info("ğŸ“º å•è§†é¢‘å±äºåˆé›†ï¼Œå°è¯•æå–å®Œæ•´åˆé›†")
            # å¦‚æœæ˜¯å•è§†é¢‘å±äºåˆé›†ï¼Œå°è¯•é€šè¿‡yt-dlpè·å–å®Œæ•´åˆé›†
            return _extract_bilibili_video_collection_via_ytdlp(url, max_videos)
        
        # ä½¿ç”¨yt-dlpæå–æ’­æ”¾åˆ—è¡¨ä¿¡æ¯ï¼ˆè®¾ç½®è¶…æ—¶ï¼‰
        logger.info("ğŸ”„ å°è¯•ä½¿ç”¨yt-dlpæå–æ’­æ”¾åˆ—è¡¨...")
        ydl_opts = {
            'quiet': True,
            'no_warnings': True,
            'extract_flat': True,  # åªæå–URLï¼Œä¸ä¸‹è½½
            'socket_timeout': 10,  # 10ç§’ç½‘ç»œè¶…æ—¶
            'retries': 0,  # ä¸é‡è¯•
            'fragment_retries': 0,  # ç‰‡æ®µä¸é‡è¯•
        }
        
        # ä½¿ç”¨çº¿ç¨‹å’Œè¶…æ—¶æ§åˆ¶
        import threading
        result = {"info": None, "error": None}
        
        def extract_playlist_thread():
            try:
                with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                    result["info"] = ydl.extract_info(url, download=False)
            except Exception as e:
                result["error"] = e
        
        # å¯åŠ¨çº¿ç¨‹
        thread = threading.Thread(target=extract_playlist_thread)
        thread.daemon = True
        thread.start()
        
        # ç­‰å¾…æœ€å¤š15ç§’
        thread.join(timeout=15)
        
        if thread.is_alive():
            logger.warning(f"âš ï¸ yt-dlpæ’­æ”¾åˆ—è¡¨æå–è¶…æ—¶: {url}")
            # å°è¯•APIæ–¹å¼
            return _extract_bilibili_collection_by_api(url, max_videos)
        
        if result["error"]:
            raise result["error"]
        
        info = result["info"]
        
        if info and 'entries' in info:
            # è¿™æ˜¯ä¸€ä¸ªæ’­æ”¾åˆ—è¡¨
            logger.info(f"âœ… yt-dlpæ£€æµ‹åˆ°æ’­æ”¾åˆ—è¡¨ï¼ŒåŒ…å« {len(info['entries'])} ä¸ªæ¡ç›®")
            for entry in info['entries'][:max_videos]:
                if entry.get('url') and entry.get('title'):
                    video_url = entry['url']
                    if not video_url.startswith('http'):
                        video_url = f"https://www.bilibili.com/video/{video_url}"
                    videos.append((video_url, entry['title']))
                    
            logger.info(f"âœ… yt-dlpæå–æˆåŠŸï¼Œè·å¾— {len(videos)} ä¸ªè§†é¢‘")
        else:
            # å°è¯•é€šè¿‡APIè·å–åˆé›†ä¿¡æ¯
            logger.info("âš ï¸ yt-dlpæœªæ£€æµ‹åˆ°æ’­æ”¾åˆ—è¡¨ï¼Œå°è¯•ä½¿ç”¨APIæ–¹å¼...")
            videos = _extract_bilibili_collection_by_api(url, max_videos)
                
    except Exception as e:
        logger.error(f"âŒ æå–Bç«™åˆé›†è§†é¢‘å¤±è´¥: {e}")
        # å¦‚æœyt-dlpå¤±è´¥ï¼Œå°è¯•APIæ–¹å¼ä½œä¸ºå¤‡é€‰
        try:
            logger.info("ğŸ”„ å°è¯•ä½¿ç”¨APIæ–¹å¼ä½œä¸ºå¤‡é€‰...")
            videos = _extract_bilibili_collection_by_api(url, max_videos)
        except Exception as api_e:
            logger.error(f"âŒ APIæ–¹å¼ä¹Ÿå¤±è´¥: {api_e}")
        
    return videos


def _extract_bilibili_video_collection_via_ytdlp(url: str, max_videos: int = 50) -> List[Tuple[str, str]]:
    """
    é€šè¿‡Bç«™APIä»å•ä¸ªè§†é¢‘æå–å…¶æ‰€å±åˆé›†çš„æ‰€æœ‰è§†é¢‘
    å‚è€ƒBilibiliDowné¡¹ç›®çš„å®ç°æ€è·¯
    """
    logger.info(f"ğŸ” å°è¯•ä»å•è§†é¢‘æå–å®Œæ•´åˆé›†: {url}")
    videos = []
    
    # æå–BVå·
    bv_match = re.search(r"BV([0-9A-Za-z]+)", url)
    if not bv_match:
        logger.error("âŒ æ— æ³•ä»URLæå–BVå·")
        return videos
    
    bv_id = f"BV{bv_match.group(1)}"
    logger.info(f"ğŸ“¹ æå–åˆ°BVå·: {bv_id}")
    
    try:
        # ä½¿ç”¨Bç«™APIè·å–è§†é¢‘ä¿¡æ¯
        logger.info("ğŸŒ é€šè¿‡Bç«™APIè·å–è§†é¢‘è¯¦ç»†ä¿¡æ¯...")
        
        import requests
        
        # è·å–è§†é¢‘åŸºæœ¬ä¿¡æ¯
        api_url = f"https://api.bilibili.com/x/web-interface/view?bvid={bv_id}"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://www.bilibili.com/'
        }
        
        # æ·»åŠ ç™»å½•cookieæ”¯æŒ
        bilibili_cookie = cookie_manager.get("bilibili")
        if bilibili_cookie:
            logger.info("ğŸª ä½¿ç”¨å·²ä¿å­˜çš„Bç«™ç™»å½•cookieè·å–åˆé›†ä¿¡æ¯")
            headers['Cookie'] = bilibili_cookie
        
        response = requests.get(api_url, headers=headers, timeout=10)
        
        if response.status_code == 200:
            data = response.json()
            
            if data.get('code') == 0 and 'data' in data:
                video_data = data['data']
                video_title = video_data.get('title', 'æœªçŸ¥æ ‡é¢˜')
                logger.info(f"ğŸ“Š è·å–åˆ°è§†é¢‘ä¿¡æ¯: {video_title}")
                                
                # æ–¹æ³•1: å¤„ç†ç•ªå‰§/ç”µå½±åˆé›†ï¼ˆseasonå­—æ®µï¼‰
                if 'season' in video_data and video_data['season']:
                    season_info = video_data['season']
                    season_id = season_info.get('season_id')
                    season_title = season_info.get('title', 'æœªçŸ¥ç•ªå‰§')
                    
                    logger.info(f"âœ… å‘ç°ç•ªå‰§åˆé›†: {season_title} (Season ID: {season_id})")
                    
                    # è·å–ç•ªå‰§çš„æ‰€æœ‰å‰§é›†
                    bangumi_api_url = f"https://api.bilibili.com/pgc/web/season/section?season_id={season_id}"
                    bangumi_headers = headers.copy()
                    if bilibili_cookie:
                        bangumi_headers['Cookie'] = bilibili_cookie
                    bangumi_response = requests.get(bangumi_api_url, headers=bangumi_headers, timeout=10)
                    
                    if bangumi_response.status_code == 200:
                        bangumi_data = bangumi_response.json()
                        
                        if bangumi_data.get('code') == 0 and 'result' in bangumi_data:
                            result = bangumi_data['result']
                            # å¤„ç†æ­£ç‰‡å’ŒèŠ±çµ®ç­‰
                            sections = result.get('section', [])
                            if not sections and 'main_section' in result:
                                sections = [result['main_section']]
                            
                            for section in sections:
                                episodes = section.get('episodes', [])
                                logger.info(f"ğŸ“¹ ç•ªå‰§ç« èŠ‚åŒ…å« {len(episodes)} ä¸ªå‰§é›†")
                                
                                for episode in episodes:
                                    if episode.get('bvid') and episode.get('long_title'):
                                        episode_url = f"https://www.bilibili.com/video/{episode['bvid']}"
                                        episode_title = f"{episode.get('title', '')} {episode.get('long_title', '')}"
                                        videos.append((episode_url, episode_title.strip()))
                                
                            if videos:
                                logger.info(f"âœ… æˆåŠŸæå–ç•ªå‰§åˆé›† {len(videos)} ä¸ªå‰§é›†")
                                return videos
                
                # æ–¹æ³•2: å¤„ç†å¤šåˆ†Pè§†é¢‘
                if 'pages' in video_data and len(video_data['pages']) > 1:
                    pages = video_data['pages']
                    logger.info(f"ğŸ“¹ å‘ç°å¤šåˆ†Pè§†é¢‘ï¼Œå…± {len(pages)} ä¸ªåˆ†P")
                    
                    base_title = video_data.get('title', 'æœªçŸ¥æ ‡é¢˜')
                    for page in pages:
                        if page.get('page') and page.get('part'):
                            # æ„é€ åˆ†Pè§†é¢‘çš„URL
                            page_url = f"https://www.bilibili.com/video/{bv_id}?p={page['page']}"
                            # ç›´æ¥ä½¿ç”¨APIè¿”å›çš„åˆ†Pæ ‡é¢˜ï¼Œä¸éœ€è¦é¢å¤–æ¸…ç†
                            # APIçš„partå­—æ®µå·²ç»æ˜¯å¹²å‡€çš„åˆ†Pæ ‡é¢˜ï¼ˆå¦‚ï¼š"63.64.2-KMEANSå·¥ä½œæµç¨‹P64"ï¼‰
                            part_title = page['part']
                            
                            # åªåšåŸºæœ¬çš„å­—ç¬¦ä¸²æ¸…ç†ï¼Œå»æ‰é¦–å°¾ç©ºæ ¼
                            cleaned_title = part_title.strip()
                            
                            logger.debug(f"ğŸ“º P{page['page']}: '{cleaned_title}'")
                            videos.append((page_url, cleaned_title))
                    
                    logger.info(f"âœ… æˆåŠŸæå–å¤šåˆ†Pè§†é¢‘ {len(videos)} ä¸ªåˆ†é›†")
                    return videos
                
                # æ–¹æ³•3: å°è¯•é€šè¿‡UPä¸»çš„å…¶ä»–è§†é¢‘æŸ¥æ‰¾ç³»åˆ—
                if 'owner' in video_data:
                    owner_mid = video_data['owner'].get('mid')
                    video_title = video_data.get('title', '')
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸ºç³»åˆ—è§†é¢‘
                    series_keywords = ['åˆé›†', 'ç³»åˆ—', 'ç¬¬ä¸€é›†', 'ç¬¬äºŒé›†', 'P1', 'P2', 'ä¸Šç¯‡', 'ä¸‹ç¯‡']
                    if any(keyword in video_title for keyword in series_keywords):
                        logger.info(f"ğŸ” æ£€æµ‹åˆ°ç³»åˆ—å…³é”®è¯ï¼Œå°è¯•è·å–UPä¸»çš„ç›¸å…³è§†é¢‘...")
                        
                        # è·å–UPä¸»çš„æŠ•ç¨¿è§†é¢‘åˆ—è¡¨
                        up_videos_api = f"https://api.bilibili.com/x/space/arc/search?mid={owner_mid}&ps=20&tid=0&pn=1&keyword=&order=pubdate"
                        up_response = requests.get(up_videos_api, headers=headers, timeout=10)
                        
                        if up_response.status_code == 200:
                            up_data = up_response.json()
                            if up_data.get('code') == 0 and 'data' in up_data and 'list' in up_data['data']:
                                up_videos = up_data['data']['list']['vlist']
                                
                                # æŸ¥æ‰¾æ ‡é¢˜ç›¸ä¼¼çš„è§†é¢‘
                                base_keywords = set(video_title.split())
                                related_videos = []
                                
                                for up_video in up_videos[:max_videos]:
                                    up_title = up_video.get('title', '')
                                    up_keywords = set(up_title.split())
                                    
                                    # è®¡ç®—æ ‡é¢˜ç›¸ä¼¼åº¦ï¼ˆç®€å•çš„å…³é”®è¯åŒ¹é…ï¼‰
                                    similarity = len(base_keywords.intersection(up_keywords)) / len(base_keywords.union(up_keywords))
                                    
                                    if similarity > 0.3 or any(keyword in up_title for keyword in series_keywords):
                                        up_bvid = up_video.get('bvid')
                                        if up_bvid:
                                            # ğŸ§¹ æ¸…ç†UPä¸»ç›¸å…³è§†é¢‘æ ‡é¢˜
                                            cleaned_up_title = smart_title_clean(up_title, platform="bilibili", preserve_episode=False)
                                            related_videos.append((f"https://www.bilibili.com/video/{up_bvid}", cleaned_up_title))
                                
                                if len(related_videos) > 1:
                                    logger.info(f"âœ… å‘ç° {len(related_videos)} ä¸ªç›¸å…³ç³»åˆ—è§†é¢‘")
                                    videos.extend(related_videos[:max_videos])
                                    return videos
                
                
                # æ–¹æ³•4: å¤„ç†UGCåˆé›†
                if 'ugc_season' in video_data and video_data['ugc_season']:
                    season_info = video_data['ugc_season']
                    season_id = season_info.get('id')
                    season_title = season_info.get('title', 'æœªçŸ¥åˆé›†')
                    
                    logger.info(f"âœ… å‘ç°UGC-Seasonåˆé›†: {season_title} (ID: {season_id})")
                    
                    # è·å–åˆé›†ä¸­çš„æ‰€æœ‰è§†é¢‘
                    if 'owner' in video_data:
                        owner_mid = video_data['owner'].get('mid', 0)
                        collection_api_url = f"https://api.bilibili.com/x/polymer/space/seasons_archives_list?mid={owner_mid}&season_id={season_id}&sort_reverse=false&page_num=1&page_size={max_videos}"
                        
                        collection_response = requests.get(collection_api_url, headers=headers, timeout=10)
                        
                        if collection_response.status_code == 200:
                            collection_data = collection_response.json()
                            
                            if collection_data.get('code') == 0 and 'data' in collection_data and 'archives' in collection_data['data']:
                                archives = collection_data['data']['archives']
                                logger.info(f"ğŸ“¹ UGCåˆé›†åŒ…å« {len(archives)} ä¸ªè§†é¢‘")
                                
                                for archive in archives:
                                    if archive.get('bvid') and archive.get('title'):
                                        video_url = f"https://www.bilibili.com/video/{archive['bvid']}"
                                        # ğŸ§¹ æ¸…ç†UGCåˆé›†æ ‡é¢˜
                                        cleaned_archive_title = smart_title_clean(archive['title'], platform="bilibili", preserve_episode=False)
                                        videos.append((video_url, cleaned_archive_title))
                                
                                logger.info(f"âœ… æˆåŠŸæå–UGCåˆé›† {len(videos)} ä¸ªè§†é¢‘")
                                return videos
                    
                    # å¤„ç†æ­£ç‰‡å’ŒèŠ±çµ®ç­‰
                    sections = season_info.get('sections', [])
                    if not sections and 'main_section' in season_info:
                        sections = [season_info['main_section']]
                    
                    for section in sections:
                        episodes = section.get('episodes', [])
                        logger.info(f"ğŸ“¹ ç•ªå‰§ç« èŠ‚åŒ…å« {len(episodes)} ä¸ªå‰§é›†")
                        
                        for episode in episodes:
                            if episode.get('bvid') and episode.get('title'):
                                episode_url = f"https://www.bilibili.com/video/{episode['bvid']}"
                                original_episode_title = f"{episode.get('title', '')}"
                                # ğŸ§¹ æ¸…ç†ç•ªå‰§é›†æ•°æ ‡é¢˜
                                cleaned_episode_title = smart_title_clean(original_episode_title, platform="bilibili", preserve_episode=False)
                                videos.append((episode_url, cleaned_episode_title.strip()))
                        
                    if videos:
                        logger.info(f"âœ… æˆåŠŸæå–ç•ªå‰§åˆé›† {len(videos)} ä¸ªå‰§é›†")
                        return videos
                    
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé›†ä¿¡æ¯ï¼Œè‡³å°‘è¿”å›å½“å‰è§†é¢‘
                logger.info("ğŸ“º æœªæ‰¾åˆ°åˆé›†ä¿¡æ¯ï¼Œè¿”å›å•è§†é¢‘")
                videos.append((url, video_title))
                return videos
                        
        # å¦‚æœAPIæ–¹æ³•å¤±è´¥ï¼Œå°è¯•yt-dlpæ–¹æ³•ï¼ˆä½œä¸ºå¤‡é€‰ï¼‰
        logger.info("ğŸ”„ APIæ–¹æ³•æœªæˆåŠŸï¼Œå°è¯•yt-dlpæ–¹æ³•...")
        
        ydl_opts = {
            'quiet': True,
            'no_warnings': True,
            'extract_flat': False,
            'socket_timeout': 10,
            'retries': 1,
        }
        
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            info = ydl.extract_info(url, download=False)
            
            if info and isinstance(info, dict):
                # å¦‚æœæœ‰entriesä¿¡æ¯ï¼Œç›´æ¥ä½¿ç”¨
                if 'entries' in info and len(info['entries']) > 1:
                    logger.info(f"ğŸ“¹ yt-dlpå‘ç°åˆé›†åŒ…å« {len(info['entries'])} ä¸ªè§†é¢‘")
                    for entry in info['entries'][:max_videos]:
                        if entry.get('webpage_url') and entry.get('title'):
                            videos.append((entry['webpage_url'], entry['title']))
                    
                    logger.info(f"âœ… yt-dlpæˆåŠŸæå– {len(videos)} ä¸ªåˆé›†è§†é¢‘")
                
                else:
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆé›†ä¿¡æ¯ï¼Œå°†å½“å‰è§†é¢‘ä½œä¸ºå•ä¸ªç»“æœè¿”å›
                    logger.info("ğŸ“º yt-dlpæœªæ‰¾åˆ°åˆé›†ä¿¡æ¯ï¼Œè¿”å›å•è§†é¢‘")
                    if info.get('webpage_url') and info.get('title'):
                        videos.append((info['webpage_url'], info['title']))
                        
    except Exception as e:
        logger.error(f"âŒ æå–åˆé›†è§†é¢‘å¤±è´¥: {e}")
        # ä½œä¸ºæœ€åçš„å¤‡é€‰ï¼Œè‡³å°‘è¿”å›åŸè§†é¢‘
        try:
            videos.append((url, "æœªçŸ¥æ ‡é¢˜"))
        except:
            pass
    
    logger.info(f"ğŸ¬ æœ€ç»ˆæå–åˆ° {len(videos)} ä¸ªè§†é¢‘")
    return videos


def _extract_bilibili_collection_by_api(url: str, max_videos: int = 50) -> List[Tuple[str, str]]:
    """
    é€šè¿‡Bç«™APIæå–åˆé›†è§†é¢‘
    """
    logger.info(f"ğŸŒ ä½¿ç”¨Bç«™APIæå–: {url}")
    videos = []
    
    try:
        # è§£æä¸åŒç±»å‹çš„Bç«™åˆé›†URL
        if "favlist" in url:
            # æ”¶è—å¤¹
            logger.info("ğŸ“ å¤„ç†æ”¶è—å¤¹é“¾æ¥...")
            fid_match = re.search(r"fid=(\d+)", url)
            if fid_match:
                fid = fid_match.group(1)
                logger.info(f"ğŸ“‚ æ”¶è—å¤¹ID: {fid}")
                api_url = f"https://api.bilibili.com/x/v3/fav/resource/list?media_id={fid}&pn=1&ps={max_videos}"
                videos = _fetch_bilibili_favlist_videos(api_url)
            else:
                logger.error("âŒ æ— æ³•ä»æ”¶è—å¤¹URLä¸­æå–fid")
                
        elif "collectiondetail" in url:
            # åˆé›†
            logger.info("ğŸ“š å¤„ç†åˆé›†é“¾æ¥...")
            sid_match = re.search(r"sid=(\d+)", url)
            if sid_match:
                sid = sid_match.group(1)
                logger.info(f"ğŸ“– åˆé›†ID: {sid}")
                api_url = f"https://api.bilibili.com/x/polymer/space/seasons_archives_list?mid=0&season_id={sid}&sort_reverse=false&page_num=1&page_size={max_videos}"
                videos = _fetch_bilibili_collection_videos(api_url)
            else:
                logger.error("âŒ æ— æ³•ä»åˆé›†URLä¸­æå–sid")
                
        elif "seriesdetail" in url:
            # ç³»åˆ—
            logger.info("ğŸ“š å¤„ç†ç³»åˆ—é“¾æ¥...")
            sid_match = re.search(r"sid=(\d+)", url)
            if sid_match:
                sid = sid_match.group(1)
                logger.info(f"ğŸ“– ç³»åˆ—ID: {sid}")
                api_url = f"https://api.bilibili.com/x/polymer/space/seasons_archives_list?mid=0&season_id={sid}&sort_reverse=false&page_num=1&page_size={max_videos}"
                videos = _fetch_bilibili_collection_videos(api_url)
            else:
                logger.error("âŒ æ— æ³•ä»ç³»åˆ—URLä¸­æå–sid")
                
        elif "watchlater" in url:
            # ç¨åå†çœ‹
            logger.info("â° å¤„ç†ç¨åå†çœ‹...")
            api_url = f"https://api.bilibili.com/x/v2/history/toview?ps={max_videos}&pn=1"
            videos = _fetch_bilibili_watchlater_videos(api_url)
            
        elif "bangumi/play/ss" in url:
            # ç•ªå‰§ç³»åˆ—
            logger.info("ğŸ­ å¤„ç†ç•ªå‰§ç³»åˆ—...")
            ss_match = re.search(r"ss(\d+)", url)
            if ss_match:
                ss_id = ss_match.group(1)
                logger.info(f"ğŸ­ ç•ªå‰§ç³»åˆ—ID: {ss_id}")
                api_url = f"https://api.bilibili.com/pgc/web/season/section?season_id={ss_id}"
                videos = _fetch_bilibili_bangumi_videos(api_url)
            else:
                logger.error("âŒ æ— æ³•ä»ç•ªå‰§URLä¸­æå–ss_id")
                
        elif "bangumi/media/md" in url:
            # ç•ªå‰§åª’ä½“
            logger.info("ğŸ­ å¤„ç†ç•ªå‰§åª’ä½“...")
            md_match = re.search(r"md(\d+)", url)
            if md_match:
                md_id = md_match.group(1)
                logger.info(f"ğŸ­ ç•ªå‰§åª’ä½“ID: {md_id}")
                # å…ˆè·å–season_idï¼Œå†è·å–å‰§é›†åˆ—è¡¨
                media_api_url = f"https://api.bilibili.com/pgc/review/user?media_id={md_id}"
                videos = _fetch_bilibili_bangumi_by_media_id(media_api_url, max_videos)
            else:
                logger.error("âŒ æ— æ³•ä»ç•ªå‰§åª’ä½“URLä¸­æå–md_id")
        else:
            logger.warning("âš ï¸ æœªè¯†åˆ«çš„Bç«™åˆé›†ç±»å‹")
                
    except Exception as e:
        logger.error(f"âŒ é€šè¿‡APIæå–Bç«™åˆé›†å¤±è´¥: {e}")
        
    return videos


def _fetch_bilibili_favlist_videos(api_url: str) -> List[Tuple[str, str]]:
    """
    è·å–Bç«™æ”¶è—å¤¹è§†é¢‘åˆ—è¡¨
    """
    logger.info(f"ğŸ“¡ è¯·æ±‚æ”¶è—å¤¹API: {api_url}")
    videos = []
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Referer': 'https://www.bilibili.com/'
        }
        
        # æ·»åŠ ç™»å½•cookieæ”¯æŒï¼Œç”¨äºè®¿é—®ç§äººæ”¶è—å¤¹
        bilibili_cookie = cookie_manager.get("bilibili")
        if bilibili_cookie:
            logger.info("ğŸª ä½¿ç”¨å·²ä¿å­˜çš„Bç«™ç™»å½•cookieè®¿é—®æ”¶è—å¤¹")
            headers['Cookie'] = bilibili_cookie
        
        response = requests.get(api_url, headers=headers)
        logger.info(f"ğŸ“¡ APIå“åº”çŠ¶æ€: {response.status_code}")
        
        data = response.json()
        logger.info(f"ğŸ“Š APIå“åº”æ•°æ®: code={data.get('code')}, message={data.get('message', 'N/A')}")
        
        if data.get('code') == 0 and 'data' in data and 'medias' in data['data']:
            medias = data['data']['medias']
            logger.info(f"ğŸ“¹ æ”¶è—å¤¹åŒ…å« {len(medias)} ä¸ªè§†é¢‘")
            
            for media in medias:
                if media.get('bvid') and media.get('title'):
                    video_url = f"https://www.bilibili.com/video/{media['bvid']}"
                    videos.append((video_url, media['title']))
            
            logger.info(f"âœ… æˆåŠŸæå– {len(videos)} ä¸ªæ”¶è—å¤¹è§†é¢‘")
        else:
            logger.error(f"âŒ æ”¶è—å¤¹APIè¿”å›é”™è¯¯: {data}")
                    
    except Exception as e:
        logger.error(f"âŒ è·å–Bç«™æ”¶è—å¤¹è§†é¢‘å¤±è´¥: {e}")
        
    return videos


def _fetch_bilibili_collection_videos(api_url: str) -> List[Tuple[str, str]]:
    """
    è·å–Bç«™åˆé›†è§†é¢‘åˆ—è¡¨
    """
    logger.info(f"ğŸ“¡ è¯·æ±‚åˆé›†API: {api_url}")
    videos = []
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Referer': 'https://www.bilibili.com/'
        }
        
        # æ·»åŠ ç™»å½•cookieæ”¯æŒï¼Œç”¨äºè®¿é—®éœ€è¦æƒé™çš„åˆé›†
        bilibili_cookie = cookie_manager.get("bilibili")
        if bilibili_cookie:
            logger.info("ğŸª ä½¿ç”¨å·²ä¿å­˜çš„Bç«™ç™»å½•cookieè®¿é—®åˆé›†")
            headers['Cookie'] = bilibili_cookie
        
        response = requests.get(api_url, headers=headers)
        logger.info(f"ğŸ“¡ APIå“åº”çŠ¶æ€: {response.status_code}")
        
        data = response.json()
        logger.info(f"ğŸ“Š APIå“åº”æ•°æ®: code={data.get('code')}, message={data.get('message', 'N/A')}")
        
        if data.get('code') == 0 and 'data' in data and 'archives' in data['data']:
            archives = data['data']['archives']
            logger.info(f"ğŸ“¹ åˆé›†åŒ…å« {len(archives)} ä¸ªè§†é¢‘")
            
            for archive in archives:
                if archive.get('bvid') and archive.get('title'):
                    video_url = f"https://www.bilibili.com/video/{archive['bvid']}"
                    videos.append((video_url, archive['title']))
            
            logger.info(f"âœ… æˆåŠŸæå– {len(videos)} ä¸ªåˆé›†è§†é¢‘")
        else:
            logger.error(f"âŒ åˆé›†APIè¿”å›é”™è¯¯: {data}")
                    
    except Exception as e:
        logger.error(f"âŒ è·å–Bç«™åˆé›†è§†é¢‘å¤±è´¥: {e}")
        
    return videos


def _fetch_bilibili_watchlater_videos(api_url: str) -> List[Tuple[str, str]]:
    """
    è·å–Bç«™ç¨åå†çœ‹è§†é¢‘åˆ—è¡¨
    """
    logger.info(f"ğŸ“¡ è¯·æ±‚ç¨åå†çœ‹API: {api_url}")
    videos = []
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Referer': 'https://www.bilibili.com/'
        }
        
        # ç¨åå†çœ‹éœ€è¦ç™»å½•cookie
        bilibili_cookie = cookie_manager.get("bilibili")
        if bilibili_cookie:
            logger.info("ğŸª ä½¿ç”¨å·²ä¿å­˜çš„Bç«™ç™»å½•cookieè®¿é—®ç¨åå†çœ‹")
            headers['Cookie'] = bilibili_cookie
        else:
            logger.warning("âš ï¸ ç¨åå†çœ‹éœ€è¦ç™»å½•ï¼Œä½†æœªæ‰¾åˆ°æœ‰æ•ˆcookie")
            return videos
        
        response = requests.get(api_url, headers=headers)
        logger.info(f"ğŸ“¡ APIå“åº”çŠ¶æ€: {response.status_code}")
        
        data = response.json()
        logger.info(f"ğŸ“Š APIå“åº”æ•°æ®: code={data.get('code')}, message={data.get('message', 'N/A')}")
        
        if data.get('code') == 0 and 'data' in data and 'list' in data['data']:
            video_list = data['data']['list']
            logger.info(f"ğŸ“¹ ç¨åå†çœ‹åŒ…å« {len(video_list)} ä¸ªè§†é¢‘")
            
            for video in video_list:
                if video.get('bvid') and video.get('title'):
                    video_url = f"https://www.bilibili.com/video/{video['bvid']}"
                    videos.append((video_url, video['title']))
            
            logger.info(f"âœ… æˆåŠŸæå– {len(videos)} ä¸ªç¨åå†çœ‹è§†é¢‘")
        else:
            logger.error(f"âŒ ç¨åå†çœ‹APIè¿”å›é”™è¯¯: {data}")
                    
    except Exception as e:
        logger.error(f"âŒ è·å–Bç«™ç¨åå†çœ‹è§†é¢‘å¤±è´¥: {e}")
        
    return videos


def _fetch_bilibili_bangumi_videos(api_url: str) -> List[Tuple[str, str]]:
    """
    è·å–Bç«™ç•ªå‰§è§†é¢‘åˆ—è¡¨
    """
    logger.info(f"ğŸ“¡ è¯·æ±‚ç•ªå‰§API: {api_url}")
    videos = []
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Referer': 'https://www.bilibili.com/'
        }
        
        # æ·»åŠ ç™»å½•cookieæ”¯æŒï¼ˆéƒ¨åˆ†ç•ªå‰§éœ€è¦å¤§ä¼šå‘˜ï¼‰
        bilibili_cookie = cookie_manager.get("bilibili")
        if bilibili_cookie:
            logger.info("ğŸª ä½¿ç”¨å·²ä¿å­˜çš„Bç«™ç™»å½•cookieè®¿é—®ç•ªå‰§")
            headers['Cookie'] = bilibili_cookie
        
        response = requests.get(api_url, headers=headers)
        logger.info(f"ğŸ“¡ APIå“åº”çŠ¶æ€: {response.status_code}")
        
        data = response.json()
        logger.info(f"ğŸ“Š APIå“åº”æ•°æ®: code={data.get('code')}, message={data.get('message', 'N/A')}")
        
        if data.get('code') == 0 and 'result' in data:
            result = data['result']
            # å¤„ç†æ­£ç‰‡å’ŒèŠ±çµ®ç­‰
            sections = result.get('section', [])
            if not sections and 'main_section' in result:
                sections = [result['main_section']]
            
            for section in sections:
                episodes = section.get('episodes', [])
                logger.info(f"ğŸ“¹ ç•ªå‰§ç« èŠ‚åŒ…å« {len(episodes)} ä¸ªå‰§é›†")
                
                for episode in episodes:
                    if episode.get('bvid') and episode.get('long_title'):
                        episode_url = f"https://www.bilibili.com/video/{episode['bvid']}"
                        episode_title = f"{episode.get('title', '')} {episode.get('long_title', '')}"
                        videos.append((episode_url, episode_title.strip()))
            
            logger.info(f"âœ… æˆåŠŸæå– {len(videos)} ä¸ªç•ªå‰§å‰§é›†")
        else:
            logger.error(f"âŒ ç•ªå‰§APIè¿”å›é”™è¯¯: {data}")
                    
    except Exception as e:
        logger.error(f"âŒ è·å–Bç«™ç•ªå‰§è§†é¢‘å¤±è´¥: {e}")
        
    return videos


def _fetch_bilibili_bangumi_by_media_id(api_url: str, max_videos: int = 50) -> List[Tuple[str, str]]:
    """
    é€šè¿‡åª’ä½“IDè·å–Bç«™ç•ªå‰§è§†é¢‘åˆ—è¡¨
    """
    logger.info(f"ğŸ“¡ è¯·æ±‚ç•ªå‰§åª’ä½“API: {api_url}")
    videos = []
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Referer': 'https://www.bilibili.com/'
        }
        
        # æ·»åŠ ç™»å½•cookieæ”¯æŒ
        bilibili_cookie = cookie_manager.get("bilibili")
        if bilibili_cookie:
            logger.info("ğŸª ä½¿ç”¨å·²ä¿å­˜çš„Bç«™ç™»å½•cookieè®¿é—®ç•ªå‰§åª’ä½“")
            headers['Cookie'] = bilibili_cookie
        
        response = requests.get(api_url, headers=headers)
        logger.info(f"ğŸ“¡ APIå“åº”çŠ¶æ€: {response.status_code}")
        
        data = response.json()
        logger.info(f"ğŸ“Š APIå“åº”æ•°æ®: code={data.get('code')}")
        
        if data.get('code') == 0 and 'result' in data and 'media' in data['result']:
            media_info = data['result']['media']
            season_id = media_info.get('season_id')
            
            if season_id:
                logger.info(f"ğŸ­ è·å–åˆ°ç•ªå‰§season_id: {season_id}")
                # ä½¿ç”¨season_idè·å–å‰§é›†åˆ—è¡¨
                season_api_url = f"https://api.bilibili.com/pgc/web/season/section?season_id={season_id}"
                videos = _fetch_bilibili_bangumi_videos(season_api_url)
            else:
                logger.error("âŒ æ— æ³•ä»ç•ªå‰§åª’ä½“ä¿¡æ¯ä¸­è·å–season_id")
        else:
            logger.error(f"âŒ ç•ªå‰§åª’ä½“APIè¿”å›é”™è¯¯: {data}")
                    
    except Exception as e:
        logger.error(f"âŒ è·å–Bç«™ç•ªå‰§åª’ä½“è§†é¢‘å¤±è´¥: {e}")
        
    return videos


def _extract_douyin_collection_videos(url: str, max_videos: int = 50) -> List[Tuple[str, str]]:
    """
    æå–æŠ–éŸ³åˆé›†ä¸­çš„è§†é¢‘
    """
    logger.info(f"ğŸµ ä½¿ç”¨æŠ–éŸ³æå–å™¨å¤„ç†: {url}")
    videos = []
    
    try:
        # æŠ–éŸ³çš„åˆé›†æå–æ¯”è¾ƒå¤æ‚ï¼Œè¿™é‡Œæä¾›ä¸€ä¸ªåŸºç¡€å®ç°
        # å®é™…ä½¿ç”¨ä¸­å¯èƒ½éœ€è¦æ ¹æ®æŠ–éŸ³çš„å…·ä½“APIè°ƒæ•´
        
        if "user/" in url:
            # ç”¨æˆ·ä¸»é¡µè§†é¢‘
            logger.info("ğŸ‘¤ å¤„ç†æŠ–éŸ³ç”¨æˆ·ä¸»é¡µ...")
            videos = _fetch_douyin_user_videos(url, max_videos)
                
        elif "hashtag/" in url:
            # è¯é¢˜è§†é¢‘
            logger.info("ğŸ·ï¸ å¤„ç†æŠ–éŸ³è¯é¢˜é¡µé¢...")
            hashtag_match = re.search(r"hashtag/([^/?]+)", url)
            if hashtag_match:
                hashtag = hashtag_match.group(1)
                logger.info(f"ğŸ·ï¸ è¯é¢˜æ ‡ç­¾: {hashtag}")
                videos = _fetch_douyin_hashtag_videos(hashtag, max_videos)
            else:
                logger.error("âŒ æ— æ³•ä»è¯é¢˜URLä¸­æå–hashtag")
        else:
            logger.warning("âš ï¸ æœªè¯†åˆ«çš„æŠ–éŸ³åˆé›†ç±»å‹")
                
    except Exception as e:
        logger.error(f"âŒ æå–æŠ–éŸ³åˆé›†è§†é¢‘å¤±è´¥: {e}")
        
    return videos


def _fetch_douyin_user_videos(user_url: str, max_videos: int = 50) -> List[Tuple[str, str]]:
    """
    è·å–æŠ–éŸ³ç”¨æˆ·çš„è§†é¢‘åˆ—è¡¨
    """
    logger.info(f"ğŸ‘¤ è·å–æŠ–éŸ³ç”¨æˆ·è§†é¢‘: {user_url}")
    videos = []
    
    try:
        from app.downloaders.douyin_downloader import DouyinDownloader
        logger.info("ğŸ”§ åˆå§‹åŒ–æŠ–éŸ³ä¸‹è½½å™¨...")
        downloader = DouyinDownloader()
        videos = downloader.get_user_collection_videos(user_url, max_videos)
        logger.info(f"âœ… æŠ–éŸ³ä¸‹è½½å™¨è¿”å› {len(videos)} ä¸ªè§†é¢‘")
        
    except Exception as e:
        logger.error(f"âŒ è·å–æŠ–éŸ³ç”¨æˆ·è§†é¢‘å¤±è´¥: {e}")
        
    return videos


def _fetch_douyin_hashtag_videos(hashtag: str, max_videos: int = 50) -> List[Tuple[str, str]]:
    """
    è·å–æŠ–éŸ³è¯é¢˜çš„è§†é¢‘åˆ—è¡¨
    """
    logger.info(f"ğŸ·ï¸ è·å–æŠ–éŸ³è¯é¢˜è§†é¢‘: {hashtag}")
    videos = []
    
    try:
        # è¯é¢˜è§†é¢‘è·å–æ¯”è¾ƒå¤æ‚ï¼Œæš‚æ—¶ç•™ä½œå ä½
        logger.warning(f"âš ï¸ è·å–è¯é¢˜ {hashtag} çš„è§†é¢‘åˆ—è¡¨ï¼ˆæš‚æœªå®ç°ï¼‰")
        
    except Exception as e:
        logger.error(f"âŒ è·å–æŠ–éŸ³è¯é¢˜è§†é¢‘å¤±è´¥: {e}")
        
    return videos


def identify_platform(url: str) -> Optional[str]:
    """
    æ ¹æ®URLè¯†åˆ«è§†é¢‘å¹³å°
    
    :param url: è§†é¢‘é“¾æ¥
    :return: å¹³å°åç§° (bilibili/douyin/youtube/kuaishou/baidu_pan) æˆ– None
    """
    logger.info(f"ğŸ” è¯†åˆ«å¹³å°: {url}")
    
    # Bç«™
    if re.search(r"bilibili\.com|b23\.tv", url):
        logger.info("âœ… è¯†åˆ«ä¸ºBç«™å¹³å°")
        return "bilibili"
    
    # æŠ–éŸ³
    elif re.search(r"douyin\.com|iesdouyin\.com", url):
        logger.info("âœ… è¯†åˆ«ä¸ºæŠ–éŸ³å¹³å°")
        return "douyin"
    
    # YouTube
    elif re.search(r"youtube\.com|youtu\.be", url):
        logger.info("âœ… è¯†åˆ«ä¸ºYouTubeå¹³å°")
        return "youtube"
    
    # å¿«æ‰‹
    elif re.search(r"kuaishou\.com", url):
        logger.info("âœ… è¯†åˆ«ä¸ºå¿«æ‰‹å¹³å°")
        return "kuaishou"
    
    # ç™¾åº¦ç½‘ç›˜
    elif re.search(r"pan\.baidu\.com", url):
        logger.info("âœ… è¯†åˆ«ä¸ºç™¾åº¦ç½‘ç›˜å¹³å°")
        return "baidu_pan"
    
    logger.warning(f"âš ï¸ æœªè¯†åˆ«çš„å¹³å°: {url}")
    return None


def extract_baidu_pan_collection_videos(url: str, max_videos: int = 50) -> List[Tuple[str, str]]:
    """
    æå–ç™¾åº¦ç½‘ç›˜ç›®å½•ä¸­çš„æ‰€æœ‰åª’ä½“æ–‡ä»¶
    
    :param url: ç™¾åº¦ç½‘ç›˜ç›®å½•é“¾æ¥
    :param max_videos: æœ€å¤§æ–‡ä»¶æ•°é‡
    :return: [(file_url, filename), ...] åˆ—è¡¨
    """
    logger.info(f"â˜ï¸ å¼€å§‹æå–ç™¾åº¦ç½‘ç›˜åª’ä½“æ–‡ä»¶: {url}")
    
    try:
        from app.downloaders.baidu_pan_downloader import BaiduPanDownloader
        
        downloader = BaiduPanDownloader()
        
        # è§£æURLç±»å‹
        share_code, extract_code = downloader.parse_share_url(url)
        
        if share_code:
            logger.info(f"ğŸ“ æ£€æµ‹åˆ°åˆ†äº«é“¾æ¥: {share_code}")
            file_list = downloader.get_file_list(share_code=share_code, extract_code=extract_code)
        else:
            # ä¸ªäººç½‘ç›˜ç›®å½•
            path = downloader.parse_path_url(url)
            logger.info(f"ğŸ“ æ£€æµ‹åˆ°ä¸ªäººç½‘ç›˜è·¯å¾„: {path}")
            file_list = downloader.get_file_list(path=path)
        
        if not file_list:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•æ–‡ä»¶")
            return []
        
        # è¿‡æ»¤åª’ä½“æ–‡ä»¶
        media_files = downloader.filter_media_files(file_list)
        
        if not media_files:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•åª’ä½“æ–‡ä»¶")
            return []
        
        # é™åˆ¶æ–‡ä»¶æ•°é‡
        media_files = media_files[:max_videos]
        
        # æ„é€ è¿”å›ç»“æœ
        videos = []
        for file_info in media_files:
            filename = file_info.get('server_filename', '')
            fs_id = str(file_info.get('fs_id', ''))
            
            # æ„é€ è™šæ‹ŸURLï¼ˆç”¨äºä»»åŠ¡è¯†åˆ«ï¼‰
            file_url = f"baidu_pan://file/{fs_id}?filename={filename}&source_url={url}"
            title = os.path.splitext(filename)[0]  # å»æ‰æ‰©å±•åä½œä¸ºæ ‡é¢˜
            
            videos.append((file_url, title))
            logger.info(f"ğŸ“„ æ‰¾åˆ°åª’ä½“æ–‡ä»¶: {title}")
        
        logger.info(f"âœ… ç™¾åº¦ç½‘ç›˜åª’ä½“æ–‡ä»¶æå–å®Œæˆï¼Œå…± {len(videos)} ä¸ªæ–‡ä»¶")
        return videos
        
    except Exception as e:
        logger.error(f"âŒ æå–ç™¾åº¦ç½‘ç›˜åª’ä½“æ–‡ä»¶å¤±è´¥: {e}")
        return []
